# 1. Introducción

Este paper titulado [Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware](https://arxiv.org/pdf/2304.13705) propone dos resultados importantes tanto a nivel de hardware como de software. La problemática que abordan es que muchos de los sistemas robóticos que requieren una manipulación fina incluyen características como **precisión**, **closed-loop feedback** y muchos grados de **hand-eye coordination**.

> Closed-loop feedback: Se refiere a la estrategia de control que el sistema utiliza como retroalimentación en tiempo real para ajustar continuamente sus acciones en respuesta a cambios en el entorno.

Los robots convencionales que logran este nivel de precisión suelen depender de **sensores avanzados y hardware costoso**. Sin embargo, si se quisiera desarrollar un sistema **low-cost**, inevitablemente se perdería la precisión necesaria para tareas de manipulación fina. Esto limita la accesibilidad de estos sistemas a laboratorios con grandes presupuestos y restringe su aplicación en entornos donde el costo es un factor a considerar.

Para abordar estos desafíos, el paper introduce **ALOHA** (A Low-cost Open-source Hardware System for Bimanual Teleoperation), un sistema de teleoperación diseñado para capturar demostraciones humanas de manipulación con alta destreza sin necesidad de hardware especializado. Complementando este sistema, se presenta **ACT** (Action Chunking with Transformers), un algoritmo de aprendizaje por imitación que permite a los robots aprender movimientos precisos a partir de datos visuales.

Debido a que nosotros usaremos un robot diferente, el **SO-ARM100**, en este artículo nos centraremos netamente en el algoritmo de aprendizaje. Sin embargo, algunos elementos como la dimensión del action-space o la cantidad de imágenes a procesar se mantendrán constantes, mientras que en la implementación propia estas son dimensiones que pueden ser modificables.

Puedes encontrar los detalles de la implementación del robot en el siguiente enlace:

[Implementación](https://www.nonhuman.site/research/wiki/SO-ARM100/1.1)

# 2. Action Chunking with Transformers:

El algoritmo a desarrollar se llama Action Chunking with Transformers y este supera a otros algoritmos existentes de *Imitation Learning*, como se verá más adelante. A continuación, hablaremos sobre el pipeline y algunas decisiones de diseño.

## 2.1 Pipeline
Para entrenar el algoritmo de ACT, primero recopilamos las demostraciones humanas usando el robot a utilizar. La información que se recopila incluye las posiciones de los joints del robot líder, las imágenes de las cámaras y las acciones de las demostraciones. Es importante usar al robot líder porque, al estar modificado, puede capturar mejor la intención real del humano, mientras que el seguidor necesita aprender estas dinámicas a través del algoritmo de *Imitation Learning*.

![Recolección de datos](https://res.cloudinary.com/dtpiuha91/image/upload/v1739396272/1_syseiy.png)

**Observations (O)**
Las observaciones que toma nuestra política están definidas por las posiciones de los joints del robot follower y las imágenes de la cámara.

> En el caso del paper, son 4 imágenes de cámaras distintas que toman como observación; sin embargo, en el proyecto de SO-ARM100 solo estamos considerando 2 imágenes de cámaras distintas. En el caso de los joints, en el paper la dimensión de estos es 14 debido a que están controlando dos brazos robóticos; en el caso de SO-ARM100, esta dimensión es de 6.

Luego entrenamos al robot para predecir la secuencia de futuras acciones dada la observación. Aquí definimos las acciones como las posiciones de joints futuros.

Entonces, en teoría, ACT trata de imitar lo que hace el humano operador en cada observación. Estos joints son manejados por los controladores PID dentro de los motores.

## 2.2 Action Chunking

Para combatir el efecto de *compounding errors* de *Imitation Learning*, donde si nuestra política comete un error, este se puede propagar al siguiente estado y generar un error acumulado, modificamos el entrenamiento de tal forma que se pueda integrar a la política de pixel-to-actions. Para esto, se utiliza el concepto de **action-chunking**. Este concepto consiste en que modificamos nuestra política de la siguiente forma: en lugar de tratar de predecir una acción en cada step como

$$
\pi(a_t|s_t)
$$

Se busca predecir la siguiente cadena de $k$ acciones. Entonces, nuestra política se plantea como

$$
\pi(a_{t:t+k}|s_t)
$$

Esto se realiza de esta forma, ya que podemos reducir el **effective horizon** que tiene nuestro modelo a una alta frecuencia; donde el **effective horizon** consiste en la cantidad de pasos que realiza nuestro modelo para poder realizar la tarea, y nos referimos a alta frecuencia debido a que previamente consideramos realizar cada acción en cada timestep. Entonces, con el action chunking, lo que conseguimos es reducir en un factor de $k$ este **effective horizon** y, además, reducir la frecuencia. Esto permite que se pueda reducir el *compounding error*, ya que se toman menos decisiones en toda la trayectoria, aprendiendo a tomar $k$ acciones por cada interacción.

![Diagrama de acción chunking](https://res.cloudinary.com/dtpiuha91/image/upload/v1739396271/2_y9isit.png)

## 2.3 Temporal Ensemble

Ahora veremos que la implementación nativa de realizar las acciones cada $k$ acciones es muy poco efectiva, ya que puede causar que los movimientos del robot sean muy bruscos y poco óptimos. En su lugar, usamos el mismo concepto de *Action Chunking*, pero añadimos una estrategia que llamaremos *Temporal Ensemble*. Esta estrategia consiste en hacer inferencia del modelo en cada timestep; sin embargo, debido a que predecimos las siguientes $k$ acciones, realizamos un promedio ponderado entre el efecto de cada acción generada previamente. Estos pesos estarán determinados por 

$$
w_i = \frac{e^{-mi}}{\sum_{j}e^{-mj}}
$$

donde $w_0$ es la acción más antigua e $i$ es el índice relativo. Además, el parámetro $m$ tendrá una función importante determinando el peso que tendrá cada acción.

**Analizando los pesos** 

Primero notemos que, como está definida la función de $w_i$, tendremos la siguiente relación:

$$
w_0 > w_1 > w_2 > ... > w_{k-1}
$$

Si tenemos solo una cierta cantidad de acciones predichas, digamos $z$, donde $1\leq z \leq k$, completaremos las siguientes acciones $w_j=0$ con $z < j$. Además, notemos que tendremos que 

$$
\sum_{j}w_j=1
$$

**Analizando el parámetro $m$**

Ahora analicemos qué influencia y rol cumple el parámetro $m$. Este parámetro, intuitivamente, indica qué tanta influencia tendrán las últimas acciones predichas para dicho timestep, ya que, por cómo está formulada la función para $w_i$, a un mayor valor de $m$, la tasa de decaimiento tiende a ser mayor. Notemos esto con el siguiente caso:

$$
\text{Con} \ m=1:\ w_0=1, w_1≈0.3679, w_2≈0.1353
$$

Probemos con un valor más pequeño:

$$
\text{Con} \ m=0.2:\ w_0=1, w_1≈0.8187, w_2≈0.6703
$$

Podemos ver entonces que cuando tenemos un $m$ más grande, la influencia que tienen las nuevas acciones para el timestep es menor, mientras que si $m$ es más pequeño, entonces las nuevas acciones tienen más influencia para el timestep actual.

![Análisis de pesos](https://res.cloudinary.com/dtpiuha91/image/upload/v1739396271/3_hf4zec.png)

## 2.4 Modelando la data humana

Notemos que la data de las demostraciones humanas puede llegar a ser ruidosa; es decir, que para la misma observación, un humano puede tener diferentes trayectorias para resolver la tarea o incluso puede tener comportamientos más estocásticos, aleatorios en partes de la trayectoria donde la precisión importa menos.

> Ejemplo: Supongamos que la tarea es colocar una taza en una repisa. Entonces, el movimiento que hace el brazo hasta tomar la taza puede ser muy aleatorio y en esta parte no importa la precisión; sin embargo, al momento de colocar la taza es donde importa más la precisión.

Por este motivo, lo que se busca es que la política aprenda a concentrarse en regiones donde la precisión es crucial.

Para afrontar este problema, se entrena como un modelo generativo; específicamente, como un [*Conditional Variational Autoencoder (CVAE)*](https://papers.nips.cc/paper_files/paper/2015/file/8d55a249e6baa5c06772297520da2051-Paper.pdf). Este tipo de arquitecturas tiene dos partes importantes: el **CVAE Encoder** y el **CVAE Decoder**. 

### 2.4.1 CVAE Encoder

> Durante las siguientes secciones nos referiremos al CVAE Encoder como Encoder y al CVAE Decoder como Decoder.

El encoder dentro de nuestra arquitectura sirve para entrenar al decoder, que vendrá a ser la política, ya que luego esta en test time será descartada, es decir, al momento de hacer inferencia. 
A nivel matemático, nuestro encoder toma la información de la observación inicial y lleva toda esta información a un espacio latente, representándola como una variable $z$. En este caso, para una mayor rapidez en el entrenamiento, se toma como entrada del encoder la posición de los joints y la secuencia de acciones de target.

Sin embargo, este proceso de representar la observación inicial a través de una variable latente $z$ no se adecúa bien a un comportamiento determinístico, entonces conseguiremos $z$ de una forma estocástica, es decir, probabilística. A continuación, la explicación.

**¿Por qué una forma estocástica?**

Imaginemos el caso en el que a un humano se le pide tomar un lápiz y escribir su nombre y que repita este proceso unas mil veces. Entonces se dará cuenta de que, aunque similares, cada trayectoria recorrida por la mano para escribir el nombre no es igual a la anterior; sin embargo, esta similaridad nos indica que pertenecen a una distribución de probabilidad; es decir, que la generación de trayectorias para que una persona escriba su nombre es un proceso estocástico. 

Entonces, de forma generalizada, si queremos imitar las demostraciones de las trayectorias de los movimientos de las personas al momento de teleoperar el brazo líder, debemos modelar este proceso como un proceso estocástico para que la variable $z$ sea una representación no fija y que pueda representar estas variaciones que tienen las demostraciones.

Entonces, la misión del encoder no será calcular $z$ de forma directa, sino que se encargará de calcular la distribución de $z$. En el paper, se matematiza esto considerando a $z$ con una distribución que está parametrizada como una diagonal Gaussiana:

$$
z \sim \prod_{i=1}^d \mathcal{N}(\mu_i,\sigma_i^2)
$$

Esto representa que cada componente está distribuido por parámetros independientes; es decir, $z_i \sim \mathcal{N}(\mu_i,\sigma_i^2)$. Entonces, nuestro Encoder para predecir la distribución bastaría con predecir cuál es la media y la varianza; es decir, los vectores $\mu$ y $\sigma^2$. Podemos plantearlo entonces como:

$$
q_{\phi}(z∣s,a)=\mathcal{N}(z∣\mu_{\phi}(s,a),\text{diag}(\sigma_{\phi}^2(s,a)))
$$

donde $\phi$ son los parámetros del Encoder y $q$ es la distribución que se obtiene a partir de la observación y las acciones.

### 2.4.2 Decoder

El Decoder toma como entrada la variable $z$ y las observaciones actuales; en este caso, sí incluimos los frames de las imágenes y las posiciones de los joints del robot para predecir la secuencia de acciones. 

Como vimos antes, el encoder ya no se utiliza en test time; es decir, al momento de hacer inferencia, no tendremos acceso a la variable $z$. Para resolver esto, al momento de hacer inferencia, seteamos a la variable $z=0$. ¿Esperen, qué? Sí, y ahora daremos la explicación para esto.

**$Z=0$**

Debido a que ahora no tenemos al encoder para poder encontrar la representación de $z$ en test time, seteamos $z=0$, ya que necesitamos un valor fijo para que al momento de realizar los movimientos del robot se comporte como un sistema determinista.

Entonces, para mayor claridad, podemos ponerlo como que el **Encoder** se encarga de aprender los diferentes estilos y tipos de trayectorias que se encuentran en las demostraciones, y esto se utiliza al **Decoder** para enseñarle este tipo de señales; sin embargo, al momento de hacer inferencia necesitamos un valor fijo para poder realizar la predicción, ya que no contamos más con la información del **Encoder**.

Entonces, deberíamos utilizar un valor representativo y, como sabemos, la media $\mu$ cumple bien esta función. Entonces, nosotros buscaremos que la media de esta variable $z$ sea igual a $0$ para poder usar este valor fijo en test time; sin embargo, ¿cómo nos aseguramos de que esta media $\mu=0$ sea la más adecuada? Para responder a esta pregunta, debemos pensar en cómo queremos entrenar a todo el modelo.

Entonces, el modelo se encarga de maximizar el log-likelihood del chunk de demostraciones. De una forma más formal, el problema de optimización es el siguiente:

$$
\min_{\theta}-\sum_{s_t,a_{t:t+k}\in D}\log \pi_{\theta}(a_{t:t+k}|s_t)
$$

Para poder minimizar esta función, utilizamos la función de pérdida estándar de los Variational Autoencoders, que tiene dos términos: $\mathcal{L}_{reconst}$ y $\mathcal{L}_{reg}$. 

**Función de reconstrucción**

El primer término es la función de pérdida que nos ayuda a medir las distancias entre las acciones predichas por el modelo y las acciones de las demostraciones. Para esto, se usa una función estándar como $MSE$. 

$$
\mathcal{L}_{reconst} = MSE(\hat{a}_{t:t+k},a_{t:t+k})
$$

donde $\hat{a}_{t:t+k}$ son las acciones predichas por el modelo y $a_{t:t+k}$ son las acciones de las demostraciones.

**Función de penalización**

Finalmente, tenemos el término de regularización que será la razón por la cual seteamos a $z=0$ en test time. Entonces, debido a que buscamos un valor fijo de $z$ para realizar la inferencia y además mantener la estabilidad del entrenamiento, entrenaremos los parámetros del **Encoder** usando la siguiente función de penalización:

$$
\mathcal{L}_{reg}= D_{KL}(q_{\phi}(z|a_{t:t+k},\overline{o}_t)||\mathcal{N}(0,I))
$$

> Nota: $\overline{o}_t$ se refiere a las observaciones que solo consisten en los joints, pero sin las imágenes.

Entonces, notemos que la función de regularización utiliza la función de la [divergencia de Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence). Esta función mide la diferencia entre dos distribuciones de probabilidad. En este caso, la función de regularización mide la diferencia entre la distribución de $z$ parametrizada por el **Encoder** y la distribución gaussiana con media $0$. Esto se hace para que, al momento de actualizar los parámetros del **Encoder**, la distribución de $z$ no se aleje tanto de la distribución gaussiana con media $0$. De esta forma, nos aseguramos de que, si bien la media de la distribución de $z$ puede ser diferente de $0$, esta llegará a ser un valor aproximado debido a esta función de penalización y, por lo tanto, representativo de esta. Esto nos garantiza que el decoder pueda tomar un buen valor de referencia y tener un comportamiento natural al momento de predecir las acciones.

**Función de pérdida**

Para la construcción final de la función de pérdida del modelo, se utiliza un hiperparámetro $\beta$ que medirá la relevancia de $z$. En este caso, tenemos que, mientras más grande sea $\beta$, menos relevancia será transmitida en $z$. 

$$
\mathcal{L} = \mathcal{L}_{reconst} + \beta\mathcal{L}_{reg}
$$

Este efecto sucede porque, si $\beta$ es demasiado grande, entonces el factor de regularización será más penalizado, lo que conllevaría a que la distribución de $z$ se parezca más a $\mathcal{N}(0,I)$ y esto haría que la información se pierda, no capturando la verdadera distribución de $z$.

# 3. Arquitectura de ACT

Como vimos previamente, la arquitectura de CVAE estaba conformada por un **CVAE Encoder** y un **CVAE Decoder**. Sin embargo, debemos tener en cuenta que el mismo **CVAE Decoder** tiene una estructura encoder-decoder. Entonces, para tener una mejor visualización gráfica de la arquitectura, a modo de ejemplo, mostramos el siguiente diagrama de entradas y salidas:
<div class="mermaid">
flowchart LR
    input1([CLS]) --> CVAE_Encoder
    input2(Joints) --> CVAE_Encoder
    input3("action sequence + pos emb") --> CVAE_Encoder
    subgraph CVAE_Encoder
        direction LR
        enc_process
    end
    subgraph CVAE_Decoder
        direction TB
        subgraph encoder[Encoder]
            enc_internal
        end 
        subgraph decoder[Decoder]
            dec_internal
        end
        encoder --> decoder
    end
    CVAE_Encoder --> z(z)
    z --> encoder
    additional1(Images Obs) --> encoder
    additional2(Joints obs) --> encoder
    decoder --> actions(Actions)
</div>

## 3.1 CVAE Encoder

Para el caso del encoder, se utiliza una arquitectura similar a la de [BERT](https://arxiv.org/pdf/1810.04805). La entrada para entrenar a esta parte del modelo estará compuesta por:
* Joints (La posición de los motores)
* El conjunto de $k$ acciones de las demostraciones, o sea, el target
* Un token especial [CLS] que sirve para condensar toda la información dentro de la salida que le corresponde y utilizar esta para predecir la media y la varianza de nuestra variable $z$.

### 3.1.1 [CLS]
Entonces, tenemos que nuestra entrada tendrá una longitud de $k+2$ entradas. Ahora veremos cómo vamos a procesar estas entradas para poder introducirlas en nuestro **CVAE Encoder**. 

Inicialmente, todos deben encontrarse en un espacio latente en común. Para conseguir esto, usaremos varias proyecciones en cada uno de los inputs de tal forma que, para el **[CLS]**, este estará representado por un vector de dimensión $512$.

![CLS|300x300](https://res.cloudinary.com/dtpiuha91/image/upload/v1739396272/4_twqby5.png)

### 3.1.2 Joints
Para la parte de los joints, tenemos que la cantidad de motores en el paper son $14$, es decir, dos brazos, por lo que tenemos un vector de dimensión $14$ que representa la posición exacta de estos. Luego, tenemos que llevarlos a esta misma dimensión usando una transformación lineal.

![Transformación de joints|400x400](https://res.cloudinary.com/dtpiuha91/image/upload/v1739396272/5_rgq5zs.png)

### 3.1.3 Target actions
Finalmente, para las acciones de las demostraciones, tendremos un proceso más elaborado. Vemos que tenemos un total de $k$ acciones; es decir, $k$ vectores de dimensión $14$. Entonces, usamos una transformación lineal para llevarlo a la dimensión $512$ y además le agregamos el [positional encoding](https://arxiv.org/pdf/1706.03762) para obtener nuestra matriz de **embedded action sequence**.

![Embedded action sequence|400x400](https://res.cloudinary.com/dtpiuha91/image/upload/v1739396272/6_ayvuhb.png)

### 3.1.4 Estructura final

Entonces, de forma general, tenemos el siguiente gráfico que nos indica la estructura general de este **CVAE Encoder**.

![Estructura general del CVAE Encoder|400x400](https://res.cloudinary.com/dtpiuha91/image/upload/v1739396272/7_g2gs0w.png)

Notemos que solo usamos el vector de salida correspondiente al vector de **[CLS]** luego de procesarlo con [bloques de atención](https://arxiv.org/pdf/1706.03762) y luego usamos una transformación lineal para conseguir tanto la media $\mu$ como la varianza $\sigma^2$. Finalmente, sampleamos $z$ para obtener nuestra representación final.

## 3.2 CVAE Decoder

Ahora hablaremos sobre la estructura del **CVAE DECODER**. Hablemos sobre las entradas a esta parte:

* $z$: Es la salida que se obtuvo del **CVAE Encoder**.
* Image observations: Son las imágenes recopiladas de las trayectorias. En el paper son 4 cámaras en total.
* Joints: La posición de los motores en los que el robot se encuentra. En este caso son $14$.

De forma similar al **CVAE Encoder**, tenemos que llevar estas entradas a un espacio en latente en común; en este caso, la dimensión también es de $512$.

### 3.2.1 $Z$
Para el caso de $z$, tenemos que, debido a que es un vector que se encuentra en una dimensión de $32$, entonces tenemos que usar una transformación lineal para llevarlo a $512$:

$$
 z' = W . z, \ \text{donde} \ W\in \mathbb{R}^{512\times 32} 
$$

### 3.2.2 Image observations
Luego, para las observaciones de las imágenes, se encuentran en una resolución de $480 \times 640$ cada una. Entonces, lo que se hace primero es usar el backbone de [ResNet18](https://arxiv.org/pdf/1512.03385) para poder procesar las imágenes y convertir cada una de la dimensión de $480 \times 640 \times 3$ hacia la dimensión de $15 \times 20 \times 520$. Luego se realiza un flatten para tenerlo en la dimensión de $300\times 512$. Finalmente, se le agrega información posicional, añadiendo un [positional encoding 2D](https://arxiv.org/pdf/2005.12872) a la secuencia de características.

Este proceso se repite para las 4 imágenes, luego las concatenamos y tenemos un tensor que tiene el tamaño de $1200\times 512$.

### 3.2.3 Joints

Para el procesamiento de los joints, la posición actual de los motores, de forma similar a lo realizado por el **CVAE Encoder**, usaremos una transformación lineal para proyectarlo a la dimensión de $512$. Entonces, tenemos que nuestros joints actuales tienen una dimensión de $14$, y usamos una capa lineal para proyectarlo a esta dimensión:

$$
J^{'}=W.J, \ \text{donde} \ W\in \mathbb{R}^{512\times 14}
$$

### 3.2.4 Estructura final

> A partir de ahora nos referiremos al **Encoder** y **Decoder**, pero que se encuentran dentro del **CVAE Decoder**.

Entonces, de forma general, tenemos que nuestra entrada tendrá un tamaño de $1202\times512$ cuando concatenamos todas nuestras entradas. Este tensor será la entrada del **Encoder**, luego realizamos un proceso de **self-attention** para obtener la salida del encoder. Esta tendrá una dimensión de $1202\times512$.

Finalmente, usamos la salida del **Encoder** para realizar **cross-attention** en el **Decoder**. Este **cross-attention** tendrá los siguientes componentes como entrada: 

* Query ($Q$): El query será un positional embedding fijo. Este tendrá la dimensión de $k\times512$, donde $k$ es la cantidad de acciones a predecir.

* Key ($K$): El key será la salida del **Encoder**; es decir, la matriz de $1202\times512$ sumada a un positional encoding.

* Value ($V$): El value será también la salida del **Encoder**, pero sin sumar nada.

Entonces, tendremos que nuestra salida tendrá un output de $k\times512$, ya que solo basta con observar la multiplicación de matrices, la cual es, de forma reducida:

$$
(Q\times K^{T})V = (k\times512).(512\times1202).(1202\times512)
$$

Finalmente, para la salida, usamos una transformación lineal para llevar esta salida a nuestro **action-space**, que es la dimensión de los motores, es decir, a la dimensión de $k\times 14$.

> Todas estas multiplicaciones las estamos realizando a nivel de matrices; en la realidad, también se considera la dimensión del batch. Para todo esto, recomendamos ver cómo funcionan los mecanismos de atención implementados en [MultiHeadAttention de Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html).

Ahora finalmente tenemos nuestra estructura general.

![Diagrama de estructura general del CVAE Decoder](https://res.cloudinary.com/dtpiuha91/image/upload/v1739396272/8_uhvpaj.png)

# 4. Algoritmos

Ahora veremos los algoritmos tanto para el entrenamiento como para la inferencia. 

## 4.1 Entrenamiento

El algoritmo es bastante directo. Notemos que la parte más importante ya fue discutida en la sección **3.4.2**, en la que tratamos el por qué de la función de pérdida y el rol de $z$ dentro del entrenamiento. Lo más importante que podríamos resaltar podría ser la estructura del dataset. Como se muestra en la figura a continuación, la data que debemos recolectar debe ser las imágenes, los joints y luego correlacionarlos con la secuencia de acciones de la teleoperación. 

![Recolección de datos](https://res.cloudinary.com/dtpiuha91/image/upload/v1739396272/1_syseiy.png)

![Algoritmo de entrenamiento](https://res.cloudinary.com/dtpiuha91/image/upload/v1739396272/9_r6v8hk.png)

## 4.2 Inferencia

Para el método de inferencia, lo que más resalta son las partes de **Temporal Ensemble**, que fue explicada en la sección de **3.3**. Además, en esta sección no usamos el **CVAE Encoder** para la inferencia; en su lugar, seteamos a $z=0$, ya que representa de manera adecuada la distribución de esta variable latente, ya que fue entrenada para que así fuese con la función de regularización.

![Algoritmo de inferencia](https://res.cloudinary.com/dtpiuha91/image/upload/v1739396272/10_f4rgw2.png)

# 5. Conclusiones

En este artículo explicamos el paper de [Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware](https://tonyzhaozh.github.io/aloha/) que nos ofrece un método rápido, low-cost y efectivo para entrenar a nuestro robot usando *Imitation Learning*. Cubrimos los aspectos más importantes y la explicación de algunas decisiones de diseño, desde la elección de arquitectura hasta el algoritmo de entrenamiento.

En mi opinión, las partes más interesantes son las de **Temporal Ensemble** y la elección de usar un **CVAE Encoder** para poder aprender el estilo de las trayectorias de las demostraciones. 

En los próximos blogs, mostraremos los resultados y la explicación del código para este proyecto. Además, probaremos nuevos métodos y arquitecturas.

# Referencias:

[1] [T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, "Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware."](https://tonyzhaozh.github.io/aloha/)

[2] [K. Sohn, X. Yan, and H. Lee, "Learning structured output representation using deep conditional generative models," in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), Dec. 2015.](https://papers.nips.cc/paper_files/paper/2015/file/8d55a249e6baa5c06772297520da2051-Paper.pdf)

[3] [J. Devlin, M.-W. Chang, K. Lee, y K. Toutanova, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding," arXiv preprint arXiv:1810.04805, 2018.](https://arxiv.org/pdf/1810.04805)

[4] [Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I., "Attention Is All You Need," Advances in Neural Information Processing Systems, vol. 30, 2017.](https://arxiv.org/pdf/1706.03762)

[5] [K. He, X. Zhang, S. Ren, and J. Sun, "Deep Residual Learning for Image Recognition," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770–778.](https://arxiv.org/pdf/1512.03385)

[6] [N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, "End-to-End Object Detection with Transformers," in Proc. Eur. Conf. Comput. Vis. (ECCV), 2020](https://arxiv.org/pdf/2005.12872)