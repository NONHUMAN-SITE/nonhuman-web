# 1. Introduccion

En esta sección, explicaremos varios detalles del paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762). Iremos parte por parte, desglosando cada sección y analizándola con el mayor detalle posible. Luego, en la siguiente sección [1.2](www.nonhuman.site/research/wiki/MIND/1.2), abordaremos la implementación en código y cómo podemos entrenar un pequeño modelo de lenguaje.


**Problema**

En el ámbito de la inteligencia artificial, las redes neuronales recurrentes (RNNs) y las redes neuronales convolucionales (CNNs) han desempeñado un papel crucial en tareas como el procesamiento del lenguaje natural (NLP) y el reconocimiento de patrones en imágenes. Estas arquitecturas, aunque revolucionarias en su momento, enfrentan limitaciones significativas que han impulsado la búsqueda de alternativas más eficientes y efectivas, culminando en la creación de los Transformers.


## 1.1 Redes Neuronales Recurrentes (RNNs)

Las RNNs fueron diseñadas específicamente para manejar datos secuenciales, como texto, audio o series temporales. Su estructura permite que la salida de un paso temporal alimente al siguiente, utilizando un estado oculto que transporta información contextual a lo largo de la secuencia. Sin embargo, esta arquitectura presenta las siguientes limitaciones principales:

1. **Computación lenta para secuencias largas:** Debido a su naturaleza secuencial, las RNNs procesan una palabra a la vez, lo que ralentiza tanto el entrenamiento como la inferencia.
2. **Desvanecimiento y explosión del gradiente:** Durante el entrenamiento, los gradientes pueden volverse demasiado pequeños (*vanishing*) o demasiado grandes (*exploding*), afectando la capacidad del modelo para aprender dependencias de largo plazo. Soluciones como LSTM y GRU mitigaron parcialmente este problema, pero no lo resolvieron completamente.
3. **Dificultad para acceder a información antigua:** Aunque las RNNs mantienen un estado oculto que transmite información previa, tienden a priorizar el contexto reciente, olvidando elementos importantes de contextos más lejanos.

**Ejemplo**

En una tarea de predicción de la próxima palabra en la oración:

*"El gato persigue al ratón toda la tarde. A pesar de sus esfuerzos, este no lo consigue."*

una RNN podría olvidar la relación entre *"gato"* y *"este"* si la oración es demasiado larga.


## 1.2 Redes Neuronales Convolucionales (CNNs)

Por otro lado, las CNNs, inicialmente diseñadas para el reconocimiento de imágenes, también se han adaptado para tareas de NLP. Estas redes utilizan filtros convolucionales que detectan patrones locales en los datos, como relaciones entre palabras cercanas en una oración. Sin embargo, presentan desventajas importantes en el manejo de secuencias largas:

1. **Falta de contextualización global:** Los filtros convolucionales son efectivos para captar patrones locales, pero tienen dificultades para modelar dependencias entre palabras distantes.  
2. **Estructuras predefinidas:** El tamaño del filtro y el número de capas deben definirse manualmente, lo que limita su capacidad para adaptarse a secuencias complejas.  
3. **Ineficiencia en tareas secuenciales:** Aunque son más rápidas que las RNNs gracias a su paralelismo, carecen de mecanismos intrínsecos para capturar dependencias temporales.  

**Ejemplo:**

En un párrafo como *"El desarrollo de las redes neuronales ha revolucionado la inteligencia artificial"*, una CNN puede identificar palabras relacionadas como *"desarrollo"* y *"redes"*, pero puede perder conexiones importantes entre *"revolucionado"* e *"inteligencia artificial"* debido a la distancia entre ellas.


## 2. Inputs: Tokens y Embeddings

Ahora hablaremos sobre conceptos básicos que serán útiles para las secciones posteriores. Estos elementos son los encargados de procesar inicialmente la entrada del texto y permitir su matematización. Como es bien conocido, en Deep Learning es necesario convertir los datos en unidades matemáticas con las que las redes neuronales puedan trabajar, generalmente representadas como matrices o vectores. Este proceso incluye dos pasos clave: **tokenización** y **embedding**.

## 2.1 Tokenizacion

La tokenización es el proceso de dividir un texto en unidades más pequeñas llamadas **tokens**. Dependiendo de la tarea y la complejidad, los tokens pueden ser:

- **Palabras**: Separa una oración en palabras individuales.  
  Ejemplo: “El gato duerme” → `["El", "gato", "duerme"]`.
- **Subpalabras o caracteres**: Se utiliza para reducir la carga computacional y manejar mejor palabras desconocidas.  
  Ejemplo: “gato” → `["ga", "to"]` o `["g", "a", "t", "o"]`.

Los métodos de tokenización más comunes incluyen:

- **Byte Pair Encoding (BPE)**: Divide las palabras en subunidades comunes, permitiendo un vocabulario compacto.
- **WordPiece** (usado por BERT): Similar a BPE, pero optimizado para modelos de lenguaje.
- **Métodos basados en caracteres**: Utilizados en tareas con lenguajes de baja frecuencia o texto ruidoso.

## 2.2 Embeddings

Un **embedding** es una representación numérica de los tokens en un espacio vectorial. La idea principal detrás de los embeddings es mapear cada token a un vector de dimensiones fijas. De esta manera, los tokens similares, ya sea por su semántica o contexto, estarán más cercanos entre sí en el espacio vectorial si usamos como métrica funciones como la **similaridad coseno**, mientras que los tokens diferentes estarán más separados.

$$
\text{cosine similarity}(u,v) = \frac{u \cdot v}{\|u\| \|v\|}
$$

Para imaginar los embeddings, podemos pensar en puntos distribuidos en un espacio 2D o 3D, donde cada punto representa un token y la proximidad entre los puntos refleja la similitud semántica. Estas representaciones son esenciales para que los modelos capturen relaciones entre palabras y contexto de manera efectiva. La misión principal es que el modelo aprenda a obtener la mejor representación vectorial de los tokens. De ahora en adelante, nos referiremos a la dimensión inicial de cada token como $d_{\text{model}}$.

**Videos recomendados sobre Embeddings:**

- [INTRO al Natural Language Processing (NLP) #2](https://www.youtube.com/watch?v=RkYuH_K7Fx4)
- [What are Word Embeddings?](https://www.youtube.com/watch?v=wgfSDrqYMJ4)


# 3. Attention

Una vez explicado cómo podemos convertir las oraciones en representaciones numéricas, ahora explicaremos uno de los conceptos más importantes del paper. Para esto, debemos preguntarnos: ¿qué significa la atención? ¿Cómo codificamos la atención? Observemos la siguiente fórmula propuesta en el paper.

Consideremos entonces las matrices $Q \in \mathbb{R}^{N\times d_k}, K \in \mathbb{R}^{N\times d_k}, V \in \mathbb{R}^{N\times d_v}$, donde $Q, K, V$ son las matrices conocidas como *Queries*, *Keys* y *Values*. Estas son obtenidas a partir de una transformación lineal después de pasar por la capa de *embedding*. Luego, en el paper codifican la atención como:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{d_k^{1/2}} \right)V
$$

Expliquemos el porqué de esta fórmula y la intuición detrás de ella. Empecemos por lo más básico.

> Nota: No colocamos el símbolo natural de la raíz cuadrada debido a problemas con el renderizador de KaTeX. Implementaremos un parche para esto en futuras versiones de la página.

## 3.1 Multiplicación de matrices

Podemos expresar la multiplicación de matrices de la siguiente forma en términos matemáticos. Sean las matrices $Q \in \mathbb{R}^{N\times d_k}$ y $K \in \mathbb{R}^{N\times d_k}$, su producto $S = QK^T$ (donde $K^T$ es la transpuesta de $K$) tendrá la dimensión $S \in \mathbb{R}^{N \times N}$. Si tomamos la fila $i$ y la columna $j$, cada componente de $S$ se puede calcular como:

$$
S_{i,j} = \sum_{k=1}^{d_k} Q_{i,k} K^{T}_{k,j}
$$

En otras palabras, estamos realizando un producto interno entre la fila $i$ de la matriz $Q$ y la fila $j$ de $K$ (debido a la transposición). Recordemos que podemos considerar cada fila de $Q$ y $K$ como la información en dimensión $d_k$ de cada token:

$$
\begin{bmatrix}
\text{token}_1 \\
\text{token}_2 \\
\vdots \\
\text{token}_N \\
\end{bmatrix}
$$

donde cada token $i$ se representa como un vector en $\mathbb{R}^{d_k}$:

$$
\text{token}_i \in \mathbb{R}^{d_k} \quad \text{(visto como un vector fila)}
$$

Al realizar el producto interno entre filas y columnas, almacenamos en el componente $S_{i,j}$ la información cruzada entre el $\text{token}_i$ y el $\text{token}_j$. De esta forma, podemos representar en esta matriz toda la información global sobre las relaciones entre los tokens de manera eficiente.

Sin embargo, esto aún no es suficiente. Ahora explicaremos por qué es necesario normalizar este resultado, es decir, dividirlo por $$d_k^{1/2}$$.


## 3.2 Normalizacion

Empecemos por un caso simple. Tomemos dos vectores $q \in \mathbb{R}^{d_k}$ y $v \in \mathbb{R}^{d_k}$. Supongamos que ambos tienen $\mu=0$ y $\sigma^2=1$. Entonces, al realizar el producto interno, la variable resultante tendrá $\mu=0$ y $\sigma^2=d_k$. Por lo tanto, a medida que la dimensión $d_k$ aumenta, el valor resultante tiene altas probabilidades de ser un número muy negativo o muy positivo. 

¿Por qué esto es un problema? Porque introduce inestabilidad, especialmente si queremos usar la función $softmax$, la cual es utilizada en nuestra fórmula de atención. Analicemos en detalle este fenómeno.

## 3.3 $Softmax$

La función $softmax$ está representada como:

$$
\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
$$

Notemos que al aplicar $softmax$ a un vector, el resultado es una distribución de probabilidad, ya que se cumple que:

$$
\sum_{i=1}^{n} \text{softmax}(x)_i = 1
$$

Sin embargo, debido a la forma de esta función, analicemos qué sucede cuando los componentes del vector $x$ toman valores muy negativos y muy positivos.

Dado que la función $softmax$ no tiene una salida escalar, su derivada está representada por el \textit{Jacobiano}, ya que recordemos que:

$$
\frac{\partial f}{\partial v}(a) = Df(a) \cdot v
$$

donde $Df(a)$ es la matriz compuesta por todas las derivadas parciales de $f$.

En el caso de la función $softmax$, tenemos que:

$$
\frac{\partial \text{softmax}(x)_i}{\partial x_j} = \text{softmax}(x)_i \cdot (\delta_{i,j} - \text{softmax}(x)_j)
$$

donde $\delta_{i,j}$ es la delta de Kronecker.

Debido a la forma de este componente del Jacobiano de la función $softmax$, podemos observar que en la mayoría de los casos su valor es cercano a $0$. Esto se puede demostrar considerando diferentes casos en los que algunos componentes del vector $x$ son muy positivos y otros muy negativos, y sustituyéndolos en la derivada parcial de la función.

Para ilustrar este fenómeno de manera práctica, realizaremos una simulación en la que generaremos vectores aleatorios con $\mu=0$ y $\sigma^2=d_k$, y analizaremos cómo la norma del Jacobiano de la función $softmax$ se reduce a medida que la dimensión $d_k$ aumenta. Esto implica que los gradientes se vuelven muy pequeños, lo que dificulta el entrenamiento del modelo. Este efecto, ya analizado en secciones anteriores, es conocido como el problema del \textit{vanishing gradient}.



```python
import numpy as np
import matplotlib.pyplot as plt

def softmax(z):
    e_z = np.exp(z - np.max(z))
    return e_z / e_z.sum()

def softmax_jacobian(z):
    s = softmax(z).reshape(-1, 1)
    return np.diagflat(s) - np.dot(s, s.T)

def compute_jacobian_norm(z):
    J = softmax_jacobian(z)
    return np.linalg.norm(J, 'fro')

num_simulations = 1000
d_k_values = np.logspace(0, 4, 20)
norms = []

for d_k in d_k_values:
    current_norms = []
    for _ in range(num_simulations):
        z = np.random.normal(loc=0, scale=np.sqrt(d_k), size=50)
        current_norms.append(compute_jacobian_norm(z))
    norms.append(np.mean(current_norms))

plt.figure(figsize=(10, 6))
plt.scatter(d_k_values, norms, alpha=0.7, color='red')
plt.xscale('log')
plt.yscale('log')
plt.xlabel('$d_k$ (escala logarítmica)', fontsize=12)
plt.ylabel('Norma de Frobenius del Jacobiano (log)', fontsize=12)
plt.title('Decaimiento de la norma del Jacobiano de Softmax con $d_k$ grande', fontsize=14)
plt.grid(True, which="both", ls="--")
```

![Decaimiento de la norma del Jacobiano de Softmax con $d_k$ grande|600x600](https://res.cloudinary.com/dtpiuha91/image/upload/v1738512008/cmrssnt8kll82vxhmdar.png)

Ahora veamos qué sucede cuando aplicamos el efecto normalizador. Para esto, basta con cambiar la línea de código:

```python
z = np.random.normal(loc=0, scale=np.sqrt(d_k), size=50)
z = z/np.sqrt(d_k)
```

![Decaimiento de la norma del Jacobiano de Softmax con $d_k$ grande|600x600](https://res.cloudinary.com/dtpiuha91/image/upload/v1738512047/ryfcogoazzxrswzkivnw.png)

Notemos que ahora, sin importar el tamaño de la dimensión $d_k$, la norma de la matriz jacobiana se mantiene estable y uniforme sin decaimientos.

## 3.4 Mecanismo

Ahora que entendimos la multiplicación de las matrices $Q$ y $K$, el efecto de la normalización por su dimensión $d_k$ y su implicancia al momento de usar el $softmax$, finalmente llegamos a la parte de ¿por qué se le llama un mecanismo de atención?

Definimos la matriz resultante de la operación de $softmax$ como

$$
S = \text{softmax}\left(\frac{Q K^T}{d_k^{1/2}}\right) \in \mathbb{R}^{N \times N}
$$

El siguiente paso para definir la atención ($Attention$) es multiplicar por la matriz $V \in \mathbb{R}^{N \times d_v}$. Sin embargo, la clave para entender este mecanismo de atención se encuentra al expresar las filas de la matriz de atención de la siguiente forma:

$$
Attention_i = \sum_{k=1}^{N} S_{i,k} V_k
$$

Podemos observar que la fila $i$ de $Attention$ no es más que una suma ponderada de las filas de $V$, ya que, debido a la función $softmax$, tenemos que $\sum_{k=1}^{N} S_{i,k} = 1 \ \forall i = 1,2,...,N$. Intuitivamente, estamos almacenando en cada fila de $Attention$ esta suma ponderada, donde $S_{i,k}$ indica cuál sería la relevancia de cada $token_k$ para el $token_i$ usando los "pesos" determinados por la matriz $S_{i,k}$.

# 4. Self-Attention

En la sección anterior explicamos el mecanismo de atención y sus fundamentos matemáticos. Ahora nos centraremos en lo que hace único al **Self-Attention**: el hecho de que las matrices $Q$, $K$ y $V$ provienen de la misma secuencia de entrada. Esta propiedad permite que cada token “mire” a todos los demás en la secuencia en un solo paso, lo que facilita el modelado de dependencias a larga distancia y la paralelización.

## 4.1 De Attention a Self-Attention

El Self-Attention sigue la misma formulación de la ecuación de atención descrita anteriormente, pero con la particularidad de que las matrices de **Query ($Q$), Key ($K$) y Value ($V$)** provienen de la misma secuencia de entrada:

$$
Q = X W_q, \quad K = X W_k, \quad V = X W_v
$$

Es decir:

$$
\text{Self-Attention}(X) = \text{softmax}\left( \frac{X W_q (X W_k)^T}{d_k^{1/2}} \right) X W_v
$$

Donde:

- $X \in \mathbb{R}^{N \times d_\text{model}}$ es la matriz de embeddings de entrada, donde $N$ es el número de tokens en la secuencia y $d_\text{model}$ es la dimensión del embedding.
- $W_q, W_k, W_v \in \mathbb{R}^{d_\text{model} \times d_k}$ son matrices de pesos aprendibles.

Esta característica permite que cada token atienda a todos los demás tokens de la secuencia, **incluyéndose a sí mismo**, de forma simultánea. Es decir, en lugar de depender de estados previos, como en una RNN, cada token puede acceder directamente a la información de todos los demás tokens en la secuencia.

## 4.2 Modelado Contextual

Uno de los principales beneficios de este enfoque es su capacidad para capturar dependencias dentro de un contexto local, lo que permite una mejor comprensión de las relaciones entre palabras cercanas en una secuencia. Cada token puede acceder a información relevante de toda la secuencia en un solo paso, sin importar su posición, lo que resulta en un procesamiento altamente eficiente.


**Ejemplo de Contexto en NLP**

Imaginemos la frase:

> "El banco aprobó el préstamo porque tenía fondos suficientes".

En un Transformer, el mecanismo de **Self-Attention** permite que la palabra *banco* reciba información tanto de *préstamo* como de *fondos*, ayudando a desambiguar si *banco* se refiere a una institución financiera o a un objeto físico. La matriz de atención $S$ visualiza estas relaciones, mostrando cómo los tokens influyen en la interpretación contextual de la frase.

Además de mejorar la contextualización local, también es eficaz para capturar dependencias a larga distancia en una secuencia. Esto se logra mediante la matriz de atención $S = QK^T$, que evalúa la similitud entre los tokens. Al aplicar $softmax$, se normalizan los valores y se calculan las ponderaciones de atención, permitiendo que cada token acceda eficientemente a información relevante de otros tokens distantes. Esto resuelve el problema del **desvanecimiento o explosión del gradiente** en las RNNs, asegurando una propagación de información efectiva sin pérdidas en el aprendizaje.

El mecanismo de **Self-Attention** puede generar modelos más *interpretables*. Al inspeccionar las distribuciones de atención, podemos observar cómo las diferentes cabezas de atención aprenden a realizar tareas específicas, a menudo relacionadas con la estructura **sintáctica** y **semántica** de las oraciones.

## 4.3 Comparacion y Analisis de su Arquitectura

La siguiente tabla del paper ilustra las ventajas arquitecturales a nivel de complejidad computacional:

| **Tipo de capa**               | **Complejidad por capa**    | **Operaciones secuenciales** | **Longitud máxima de la dependencia** |
|---------------------------------|-----------------------------|-----------------------------|---------------------------------------|
| **Self-Attention**             | $O(n^2 \cdot d)$            | $O(1)$                       | $O(1)$                                |
| **Recurrente (RNN)**           | $O(n \cdot d^2)$            | $O(n)$                       | $O(n)$                                |
| **Convolucional (CNN)**        | $O(k \cdot n \cdot d^2)$    | $O(1)$                       | $O(\log_k(n))$                        |
| **Self-Attention (restringido)**| $O(r \cdot n \cdot d)$      | $O(1)$                       | $O(n/r)$                              |

### 4.3.1 Complejidad computacional

Una capa conecta todas las posiciones de la secuencia con una complejidad computacional de $O(n^2 \cdot d)$, donde $n$ es la longitud de la secuencia y $d$ es la dimensión del embedding. Esta operación es eficiente para secuencias de longitud moderada, ya que las matrices de atención se calculan en paralelo, lo que permite procesar todos los tokens simultáneamente, optimizando el uso de GPUs y reduciendo los tiempos de entrenamiento. Esto facilita el manejo de grandes lotes de datos y el entrenamiento de modelos con secuencias largas de manera más eficiente.

En comparación:

- **Recurrentes**:  
  Las **RNNs** tienen una complejidad de $O(n \cdot d^2)$, debido a que la información debe propagarse de manera secuencial (un token a la vez) a través de los estados ocultos, lo que las hace menos eficientes, especialmente cuando la longitud de la secuencia es grande. 
- **Convolucionales**:  
  Por otro lado, las **CNNs** tienen una complejidad de $O(k \cdot n \cdot d^2)$, donde $k$ es el tamaño del filtro. Aunque son más rápidas que las RNNs para secuencias largas, su capacidad para capturar dependencias a largo plazo es limitada por el tamaño del filtro, siendo menos efectivas que **Self-Attention**.

### 4.3.2 Mejora en secuencias largas

Aunque **Self-Attention** es eficiente en secuencias cortas, su complejidad cuadrática $O(n^2)$ puede ser un reto en secuencias largas. Una solución es la atención restringida, donde cada token solo se conecta a un vecindario local de tamaño $r$, reduciendo la complejidad a $O(r \cdot n \cdot d)$. Esto mejora la eficiencia sin sacrificar demasiado la capacidad de aprender dependencias importantes, equilibrando la eficiencia computacional y la captura de dependencias a largo plazo.

### 4.3.3 Longitud maxima de la dependencia en **Self-Attention**

En **Self-Attention**, la longitud máxima de la dependencia es $O(1)$, lo que significa que cada token puede atender a todos los demás de manera directa, sin importar su posición en la secuencia. Esto le permite capturar relaciones globales de manera eficiente.

En comparación:

- **RNN**: La longitud de la dependencia es $O(n)$, lo que implica una propagación secuencial de la información a través de estados ocultos.
- **CNN**: La longitud de la dependencia depende del tamaño del filtro y es $O(\log_k(n))$, lo que limita la captación de dependencias a largo plazo sin aumentar la profundidad de la red.
- **Self-Attention restringido**: Limita la dependencia a un vecindario local de tamaño $O(n/r)$, mejorando la eficiencia en secuencias largas.



# 5. Multi-Head Attention: Atendiendo a Múltiples Perspectivas

En el mecanismo de **attention** tradicional, cada token de la secuencia se relaciona con otros tokens en función de su similitud, utilizando la operación de *scaled dot-product attention*. Sin embargo, cuando este proceso se realiza con una sola cabeza de atención, el modelo puede tener dificultades para captar distintos significados y relaciones complejas entre palabras. Por ejemplo, en la frase *“El banco aprobó el préstamo”*, la palabra *banco* puede significar una institución financiera o un asiento. Depender del contexto correcto es clave para evitar ambigüedades.

Para abordar esta limitación, el mecanismo de **Multi-Head Attention** divide la atención en varias "cabezas" paralelas, permitiendo que cada una se enfoque en distintos aspectos semánticos y gramaticales de la secuencia. Por ejemplo, una cabeza puede identificar relaciones sustantivas, como entre "banco" y "préstamo", mientras que otra puede enfocarse en el significado adjetivo o en cómo se desambiguan palabras polisémicas.

## 5.1 Definicion y Formulacion Matematica

El **Multi-Head Attention** se basa en la idea de ejecutar múltiples operaciones de atención en paralelo, donde cada una trabaja con proyecciones de menor dimensión del espacio original. Matemáticamente, cada cabeza de atención se define como:

$$
\text{head}_i = \text{Attention}(Q W^Q_i, K W^K_i, V W^V_i)
$$

donde las proyecciones son matrices de parámetros aprendidas durante el entrenamiento:

- $ W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k} $
- $ W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k} $
- $ W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v} $
- $ W^O \in \mathbb{R}^{h d_v \times d_{\text{model}}} $

Estas matrices proyectan las entradas $ Q $, $ K $ y $ V $ en subespacios de menor dimensión, permitiendo que cada cabeza procese la información de manera independiente. La relación entre las dimensiones para mantener la misma complejidad que al usar una sola cabeza es la siguiente:

$$
d_k = d_v = \frac{d_\text{model}}{h}
$$

Al final, los resultados de todas las cabezas se concatenan y se proyectan nuevamente al espacio original mediante una matriz $ W^O $:

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
$$

De esta manera, el modelo puede aprender y capturar múltiples relaciones entre palabras dentro de la secuencia desde distintos subespacios de representación. **Multi-Head Attention** permite que el modelo atienda conjuntamente a información de diferentes subespacios en distintas posiciones, evitando la pérdida de información que ocurriría si solo se utilizara una única cabeza de atención.

## 5.2 Flujo de Procesamiento y Explicacion de la Imagen

A continuación, explicamos el flujo del **Multi-Head Attention** utilizando la imagen adjunta:

![Multi-Head Attention](https://res.cloudinary.com/dtpiuha91/image/upload/v1738463404/multiheadattention_hapac4.png)

Primero, la secuencia de entrada, representada por una matriz de dimensiones $ \text{seq} \times d_\text{model} $, pasa por tres proyecciones lineales, transformándose en las matrices $ Q $, $ K $ y $ V $. Estas proyecciones son esenciales para calcular la atención, ya que representan los tokens en diferentes perspectivas semánticas.

Las matrices resultantes $ Q $, $ K $ y $ V $ se dividen en múltiples "cabezas" de atención. Cada cabeza opera en un subespacio de menor dimensión $ d_k = d_\text{model}/h $, permitiendo que el modelo procese varias relaciones simultáneamente. Este paso es crucial para captar tanto relaciones cercanas como dependencias a largo plazo.

Cada cabeza calcula la atención utilizando la fórmula de *scaled dot-product attention*:

$$
\text{Attention}(Q', K', V') = \text{softmax}\left(\frac{Q' {K'}^T}{d_k^{1/2}}\right)V'
$$

El cálculo del producto punto entre $ Q' $ y $ K' $ mide la similitud entre tokens, y el resultado es normalizado mediante la función $ \text{softmax} $. Esto asegura que cada token se relacione de manera ponderada con los demás, asignando más peso a los tokens más relevantes.

Una vez calculada la atención en cada cabeza, los resultados se concatenan y se proyectan al espacio original mediante la matriz $ W^O $. Esto da como resultado una representación enriquecida de la secuencia, en la que se han capturado múltiples relaciones semánticas.

---

## 5.3 Explicacion del Grafico y Ventajas del Multi-Head Attention

El gráfico adjunto ilustra el flujo completo del mecanismo **Multi-Head Attention**, desde la entrada inicial hasta la representación final enriquecida. Este mecanismo optimiza la forma en la que el Transformer capta relaciones entre tokens dentro de una secuencia, permitiendo que varias "cabezas" trabajen de manera independiente para luego combinar sus resultados. A continuación, detallamos los pasos esenciales del proceso.

### Paso 1: Entrada inicial y proyeccion en matrices $Q$, $K$ y $V$

La secuencia de entrada, representada como una matriz de dimensión $\text{seq} \times d_\text{model}$, se proyecta mediante transformaciones lineales en tres matrices: $Q$, $K$ y $V$. Estas proyecciones convierten los tokens en representaciones internas que facilitan la evaluación de sus relaciones contextuales y semánticas.

### Paso 2: División en multiples cabezas

Las matrices $Q$, $K$ y $V$ se dividen en varias "cabezas" de atención independientes. En la imagen se muestran 6 cabezas, aunque este número puede variar según la configuración del modelo. Cada cabeza trabaja en un subespacio de menor dimensión $d_k = d_\text{model}/h$ y se especializa en captar relaciones específicas. Por ejemplo, una cabeza puede enfocarse en relaciones sintácticas como sujeto y predicado, mientras que otra puede captar asociaciones entre adjetivos y sustantivos.

* Ventaja: Este proceso de división permite que el modelo capture múltiples perspectivas del contexto sin interferencias. Por ejemplo, una cabeza puede identificar relaciones gramaticales, mientras que otra se centra en desambiguar palabras polisémicas. Esta especialización es crucial para tareas donde los significados dependen fuertemente del contexto.

---

### Paso 3: Calculo del *scaled dot-product attention*

Cada cabeza realiza el producto punto entre sus matrices proyectadas $Q'$ y $K'$ para medir la similitud entre los tokens. El resultado se divide por $ d_k^{1/2}$ para evitar valores extremos que afecten el rendimiento del $softmax$ y sus gradientes. Luego, la función $softmax$ transforma estas similitudes en una distribución de probabilidad, asignando mayor peso a los tokens más relevantes.

$$
\text{Attention}(Q', K', V') = \text{softmax}\left(\frac{Q' {K'}^T}{d_k^{1/2}}\right)V'
$$

### Paso 4: Ponderacion de los valores $V'$ 

La matriz de atención resultante del $softmax$ se utiliza para ponderar los valores en $V'$, generando una representación contextualizada de cada token. De este modo, cada token incorpora información relevante de otros tokens de la secuencia. Por ejemplo, si la palabra *banco* se encuentra en el contexto de *préstamo*, la ponderación favorecerá su interpretación como una institución financiera.

* Ventaja: Esta ponderación permite al modelo captar relaciones tanto locales como a larga distancia de manera eficiente, sin necesidad de procesar la secuencia de forma secuencial como ocurre en las RNNs. Así, cada token puede acceder de forma directa a la información relevante de otros tokens, resolviendo el problema del desvanecimiento de información en secuencias largas.

### Paso 5: Concatenacion y proyeccion final

Las salidas de todas las cabezas de atención se concatenan y pasan por una última proyección lineal mediante la matriz $W^O$. Esto genera una representación final de la secuencia, en la que se combinan múltiples relaciones semánticas, gramaticales y contextuales.

Ventaja:La combinación de múltiples cabezas permite sintetizar la información desde distintas perspectivas, generando una representación rica y completa. Este proceso mejora el rendimiento en tareas complejas, como la traducción automática o el análisis de dependencias sintácticas, al integrar tanto relaciones específicas como contextos globales.


# 6. Positional Encodings

En **Transformers**, uno de los desafíos clave es cómo representar el orden de las palabras. A diferencia de las redes recurrentes (RNN), el mecanismo **self-attention** procesa los tokens en paralelo, lo que elimina la secuencialidad implícita. Sin embargo, el orden de los tokens es esencial para el sentido lingüístico. Aquí es donde entran en juego los **Positional Encodings**.

## 6.1. ¿Por que necesitamos Positional Encodings?

En arquitecturas como las RNN, el orden de las palabras está integrado en el proceso recurrente, ya que las palabras se procesan secuencialmente. En cambio, en los Transformers, el procesamiento paralelo hace que el modelo pierda noción de la secuencia a menos que se introduzca explícitamente. Esto podría resultar en ambigüedades, donde frases como “Alicia come manzanas” y “Manzanas come Alicia” serían indistinguibles.

Para evitar esto, es necesario incorporar explícitamente la información de posición en los embeddings de entrada. Un buen método para este propósito debe cumplir con los siguientes criterios:
1. Producir una codificación única para cada posición de la palabra en la oración.
2. Mantener la consistencia en las distancias entre posiciones, independientemente de la longitud de las secuencias.
3. Generalizar a secuencias más largas que las vistas durante el entrenamiento.
4. Ser determinista y no depender de aprendizaje adicional.

## 6.2. Fundamentos del Encoding sinusoidal

Según Vaswani et al. (*Attention Is All You Need*) y se discute también en *Transformer Architecture: The Positional Encoding* (Kazemnejad, 2019), los Positional Encodings se calculan usando funciones sinusoidales, que garantizan una representación continua y única para cada posición:

$$
PE(pos, 2i) = \sin(pos \cdot \omega_i), \quad PE(pos, 2i+1) = \cos(pos \cdot \omega_i)
$$

Donde:

$$
\omega_i = 1 / (10000^{2i / d_{model}})
$$

Estas funciones alternan senos y cosenos con frecuencias decrecientes a lo largo de las dimensiones del vector, creando una "firma" única para cada posición. La codificación resultante se suma a los embeddings de los tokens, manteniendo la dimensionalidad fija.


## 6.3 Intuicion basica

El encoding sinusoidal puede entenderse como una versión continua de un sistema binario. En un contador binario, cada bit cambia más lentamente que el anterior: el bit menos significativo alterna en cada paso, el siguiente cambia cada dos pasos, y así sucesivamente. De manera similar, en el encoding sinusoidal, las frecuencias más altas representan posiciones cercanas, mientras que las frecuencias más bajas reflejan relaciones a mayor distancia.

Esta superposición de frecuencias genera un patrón único para cada posición, lo que permite al modelo distinguirlas de forma efectiva.

Aquí tienes un video efectivo de cómo podemos tener la intuición de cómo se ve este mecanismo:

[Video de positional encoding](https://www.youtube.com/watch?v=T3OT8kqoqjc)

## 6.4 Propiedades clave del encoding sinusoidal

El encoding sinusoidal tiene propiedades que lo hacen particularmente adecuado para Transformers:
- **Relaciones relativas:** Permite que el modelo aprenda fácilmente relaciones posicionales relativas, ya que $$ PE(pos + k) $$ puede representarse como una función lineal de $$ PE(pos) $$.
- **Simetría y decaimiento:** Las distancias entre posiciones consecutivas son simétricas y disminuyen suavemente con el tiempo, facilitando el aprendizaje de patrones a distintas escalas.
- **Generalización:** Gracias a su naturaleza periódica, el encoding puede extrapolar a secuencias más largas que las vistas en el entrenamiento.

Estas características lo convierten en un método robusto y eficiente para incorporar información posicional.

## 6.5 Incorporacion en el Transformer

En la arquitectura Transformer, el encoding posicional se suma al embedding de cada token antes de que sea procesado por las capas de atención:

$$
x'_{pos} = x_{pos} + PE(pos)
$$

Este diseño permite que el modelo procese simultáneamente la semántica y la posición sin aumentar la dimensionalidad ni requerir más parámetros.

**¿Por que sumar en lugar de concatenar?**

La suma asegura que la dimensionalidad del vector de entrada $$ d_{model} $$ permanezca constante, reduciendo la complejidad computacional y evitando interferencias innecesarias entre la información semántica y posicional. Además, las [conexiones residuales](https://arxiv.org/pdf/1512.03385) de la arquitectura preservan la información posicional a lo largo de las capas.

# 7. Masking

El **masking** se utiliza para impedir que un token preste atención a tokens futuros; es decir, que los tokens futuros no tengan ningún efecto en la predicción de un token anterior, preservando así la propiedad auto-regresiva. En la fase de atención escalada (*scaled dot-product attention*), se introducen máscaras que asignan el valor $-\infty$ a las posiciones correspondientes a tokens futuros. De esta forma, al aplicar el $softmax$, dichos valores se convierten en 0, eliminando cualquier influencia de tokens que aún no deberían ser considerados.

## 7.1 Representacion de la Mascara

Si consideramos la interacción entre queries $$ Q $$ y keys $$ K $$ para una secuencia de longitud $$ N $$, la matriz de puntajes se modifica de la siguiente manera:

$$
\begin{bmatrix}
a_{1,1} & -\infty & -\infty & \cdots & -\infty \\
a_{2,1} & a_{2,2} & -\infty & \cdots & -\infty \\
a_{3,1} & a_{3,2} & a_{3,3} & \cdots & -\infty \\
\vdots  & \vdots  & \vdots  & \ddots & \vdots \\
a_{N,1} & a_{N,2} & a_{N,3} & \cdots & a_{N,N}
\end{bmatrix}
$$

Notemos que cada componente de la fila $$ i $$ puede ser vista como el efecto que tiene cada token $$ j $$ sobre el token $$ i $$, ya que al multiplicar

$$
Q.K^T
$$

tenemos que $$ a_{i,j} = Q_i.K_j $$, donde el subíndice indica el número de fila. Entonces, el componente $$ a_{i,j} $$ guarda la información de la atención que le da el $$ token_i $$ al $$ token_j $$.

De esta forma, al realizar este masking nos aseguramos de que solo los tokens previos al token correspondiente tengan efecto. En el siguiente blog veremos con mayor atención la implementación de esta técnica.

## 7.2 Efecto del $Softmax$

Después de aplicar la función $softmax$, los valores $$ -\infty $$ se transforman en $$ 0 $$, dando como resultado una matriz en la que los tokens futuros no influyen en la atención. Por ejemplo, la matriz de atención resultante se vería así:

$$
\begin{bmatrix}
p_{1,1} & 0 & 0 & \cdots & 0 \\
p_{2,1} & p_{2,2} & 0 & \cdots & 0 \\
p_{3,1} & p_{3,2} & p_{3,3} & \cdots & 0 \\
\vdots  & \vdots  & \vdots  & \ddots & \vdots \\
p_{N,1} & p_{N,2} & p_{N,3} & \cdots & p_{N,N}
\end{bmatrix}
$$

# 8. Cierre

En el siguiente blog veremos cómo podemos implementar todo lo mencionado previamente en código y, además, veremos también el proceso que se debe seguir para poder entrenar un pequeño modelo de lenguaje desde $ 0 $.

[Blog de implementación](www.nonhuman.site/wiki/MIND/1.2)

## Referencias

1. [Vaswani, Ashish, et al. "Attention Is All You Need". *NeurIPS*, 2017.](https://arxiv.org/abs/1706.03762)
2. [Kazemnejad, Amirhossein. "Transformer Architecture: The Positional Encoding". *kazemnejad.com*, 2019.](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
3. [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909v5)
4. [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385).
