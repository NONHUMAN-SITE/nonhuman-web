# 1. Introduccion

En esta sección, explicaremos la implementación de las partes fundamentales del Transformer y cómo podemos utilizarlo para crear un modelo de lenguaje básico. No implementaremos el modelo utilizado para la tarea de traducción descrita en el paper original, ya que nuestro enfoque es demostrar cómo este trabajo sirve como base para la construcción de modelos de lenguaje como ChatGPT.

Todo el código y los scripts implementados en esta sección están disponibles en el repositorio oficial de NONHUMAN:

[Repositorio de NONHUMAN](https://github.com/NONHUMAN-SITE/MIND/tree/main/1.AttentionIsAllYouNeed)

Aquí también encontrarás instrucciones detalladas para instalar las dependencias necesarias y entrenar este modelo de lenguaje en una carpeta con archivos PDF de tu elección. Los requerimientos de hardware no son elevados; basta con una tarjeta gráfica con al menos 6 GB de RAM. Además, si dispones de más recursos, puedes modificar los hiperparámetros para optimizar el tiempo de entrenamiento.


## 1.1 Objetivo

Creo que, antes de empezar, es importante tener en cuenta cuál debería ser el objetivo de nuestro modelo de lenguaje. La idea es que, si tenemos una secuencia de tokens, el modelo debe aprender a predecir el siguiente. Además, queremos que tenga un comportamiento autoregresivo, lo que significa que este token predicho será utilizado para predecir el siguiente. Lo podemos ver de la siguiente forma:

$$
t_N = LLM(\{t_i\}_{i=1}^{N-1}) \to t_{N+1} = LLM(\{t_i\}_{i=1}^{N})
$$

Sin embargo, más adelante veremos un concepto importante como el `context_length`, el cual será la variable que nos indicará cuántos tokens puede utilizar como máximo nuestro modelo de lenguaje para realizar inferencia.


# 2. Arquitectura

Antes de revisar el algoritmo de entrenamiento, abordaremos la arquitectura de nuestro modelo de lenguaje, centrándonos principalmente en el Transformer.

## 2.1 Positional Encoding + Embedding

Como se menciona en el paper *Attention is All You Need*, existen dos componentes principales: el **Embedding** y el **Positional Encoding**. En la sección anterior, ya discutimos la importancia de cada uno.

### 2.1.1 Embedding

En cuanto al **Embedding**, su relevancia radica en cómo podemos obtener la representación vectorial de cada token de la forma más eficiente posible. Funciona como una *tabla de búsqueda* (`look-up table`) en la que podemos asociar cada token con un vector específico.

Para implementar esto, necesitamos dos variables importantes: `num_embeddings` y `embedding_dim`.

* `num_embeddings`: Es la cantidad de tokens que almacenaremos dentro del *embedding*.
* `embedding_dim`: Es la dimensión final que tendrá nuestro vector.

Así, podemos visualizar el **Embedding** como una matriz de dimensiones `num_embeddings` x `embedding_dim`. El proceso consiste en que, para cada token, se obtiene el índice correspondiente y se extrae el vector asociado, es decir, se selecciona la fila correspondiente en la matriz.

Puedes encontrar más detalles sobre la implementación en PyTorch en el siguiente enlace:

[Documentación de `torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)



### 2.1.2 Positional Encoding

Como vimos en el blog anterior, debido a la naturaleza del transformer, necesitamos insertar información sobre la posición de los tokens, ya que no estaremos utilizando RNNs, las cuales capturan esta información de forma natural. En el paper, la solución que proponen es la siguiente:

$$
PE(pos,2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
$$

$$
PE(pos,2_{i+1}) = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
$$

donde $pos$ es la posición y $i$ es la dimensión correspondiente. Podemos ver a $PE$ como una matriz con dimensiones `num_tokens` x `embedding_dim`. La implementación de esto se encuentra en el script `model.py`, como un método de la clase del modelo de lenguaje, donde toma la dimensión del embedding y la cantidad de tokens que entrarán.


```python

def positional_encoding(self,seq_len:int, d_model:int):
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)

        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe = torch.zeros(seq_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        return pe
```

### 2.1.3 Procesamiento inicial

Tomando en cuenta estas dos partes del modelo, el proceso que se realiza es el siguiente: luego de usar el **Tokenizer** (del cual hablaremos más adelante) y obtener los índices de los tokens, los procesamos utilizando el embedding para obtener su representación vectorial. Posteriormente, agregamos a esta matriz la matriz de **Positional Encoding** para incorporar la información posicional correspondiente.

Podemos verlo de forma más formal. Sea $I \in \mathbb{R}^{T}$, donde $I$ será la lista de los $T$ tokens. Entonces:

$$
X = Embedding(I) + PE
$$

donde:

$$
X \in \mathbb{R}^{N \times d_{model}}
$$

## 2.2 Head

Ahora pasaremos a hablar sobre la implementación del mecanismo de atención del transformer. Comenzaremos implementando la clase para un solo `Head` y luego reutilizaremos este módulo para implementar el `MultiHead`.

Una vez que hemos procesado los tokens mediante el **Embedding** y el **Positional Encoding**, obtenemos la representación vectorial y posicional de nuestros tokens, $X$. A continuación, implementaremos el mecanismo de atención. Para ello, necesitamos obtener tres representaciones diferentes de nuestra matriz $X$, que son $Query$, $Key$ y $Value$. Para esto, basta con obtenerlas mediante una transformación lineal. 

$$
Q = X.W_Q \quad \text{donde } W_Q \in \mathbb{R}^{d_{model} \times d_k}
$$

$$
K = X.W_K \quad \text{donde } W_K \in \mathbb{R}^{d_{model} \times d_k}
$$

$$
V = X.W_V \quad \text{donde } W_V \in \mathbb{R}^{d_{model} \times d_v}
$$

Para implementar este tipo de transformaciones, basta con usar la capa `nn.Linear` de PyTorch, configurando el parámetro `bias=False`.

[PyTorch nn.Linear documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)

Luego, la implementación de la operación para calcular la **Attention** es bastante directa, ya que involucra operaciones básicas como la multiplicación de matrices y la división por un escalar.

$$
Attention(Q,K,V) = softmax\left(\frac{QK^T}{d_k^{1/2}}\right)V
$$

Sin embargo, uno podría pensar que basta con multiplicar dichas matrices y realizar estas operaciones. No obstante, como se menciona en el paper, debemos aplicar un proceso llamado **Masking**.


### 2.2.1 Masking

Como explicamos previamente en el blog anterior, este proceso se utiliza para que, al momento de computar la atención entre todos los tokens, es decir, una vez realizadas las operaciones de

$$
\frac{QK^T}{d_k^{1/2}}
$$

lo que sucede es que, al multiplicar estas matrices, obtenemos como resultado una matriz de dimensión $N \times N$. Sin embargo, como vimos en el blog anterior, esta matriz contiene la información de la atención entre todos los tokens, debido a que la multiplicación de matrices no es más que el producto interno entre vectores. Así, cada componente $i,j$ de esta matriz será un producto interno entre la fila $Q_i$ y la fila $K_j$, ya que la vemos transpuesta.

Sin embargo, debemos preguntarnos: ¿esto es válido? Notemos que estamos capturando también la atención entre el $token_i$ y el $token_j$ con $j > i$; es decir, el $token_i$ le presta atención a un token que se encuentra en el futuro, lo cual no debería ser posible, ya que nuestro sistema funciona de tal manera que tenemos que predecir el siguiente token basado únicamente en los anteriores. Por lo tanto, debemos aplicar una técnica de **masking** para que esto sea posible. Analicemos primero el resultado en código.


```python
import torch
import torch.nn.functional as F
torch.manual_seed(42)

N = 7
d_k = 5

Q = torch.rand((N,d_k))
K = torch.rand((N,d_k))

W = Q @ K.T
W = F.softmax(W, dim=1)

print(f"QUERY:\n{Q}")
print(f"KEY:\n{K}")
print(f"W:\n{W}")
```

**Output**
```python
QUERY:
tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904],
        [0.6009, 0.2566, 0.7936, 0.9408, 0.1332],
        [0.9346, 0.5936, 0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739, 0.2666, 0.6274],
        [0.2696, 0.4414, 0.2969, 0.8317, 0.1053],
        [0.2695, 0.3588, 0.1994, 0.5472, 0.0062],
        [0.9516, 0.0753, 0.8860, 0.5832, 0.3376]])
KEY:
tensor([[0.8090, 0.5779, 0.9040, 0.5547, 0.3423],
        [0.6343, 0.3644, 0.7104, 0.9464, 0.7890],
        [0.2814, 0.7886, 0.5895, 0.7539, 0.1952],
        [0.0050, 0.3068, 0.1165, 0.9103, 0.6440],
        [0.7071, 0.6581, 0.4913, 0.8913, 0.1447],
        [0.5315, 0.1587, 0.6542, 0.3278, 0.6532],
        [0.3958, 0.9147, 0.2036, 0.2018, 0.2018]])
W:
tensor([[0.1866, 0.2118, 0.1440, 0.0839, 0.2004, 0.0822, 0.0910],
        [0.1972, 0.2211, 0.1397, 0.0883, 0.1824, 0.1035, 0.0677],
        [0.2220, 0.2442, 0.1174, 0.0688, 0.1546, 0.1190, 0.0739],
        [0.1897, 0.1915, 0.1463, 0.0893, 0.1487, 0.1152, 0.1193],
        [0.1570, 0.1868, 0.1582, 0.1232, 0.1814, 0.0962, 0.0972],
        [0.1588, 0.1677, 0.1555, 0.1207, 0.1758, 0.1067, 0.1148],
        [0.2261, 0.2320, 0.1125, 0.0699, 0.1630, 0.1312, 0.0653]])
```

Notemos que nuestra matriz resultante `W` contiene información de atención que le da el $token_i$ al $token_j$. Para solucionar este problema, y como se menciona en el paper, necesitamos eliminar estos valores que no son válidos y forzar a que el $token_i$ solo mire a los tokens que son $\leq i$.

Para esto, pondremos como valor $-\infty$ a las posiciones inválidas, de manera que, al aplicar $softmax$, estos se conviertan en $0$. En código esto se puede ver como:


```python
import torch
import torch.nn.functional as F
torch.manual_seed(42)

N = 7
d_k = 5

Q = torch.rand((N,d_k))
K = torch.rand((N,d_k))

W = Q @ K.T
tril = torch.tril(torch.ones(N, N))
W = W.masked_fill(tril == 0, float('-inf'))
W = F.softmax(W, dim=1)

print(f"QUERY:\n{Q}")
print(f"KEY:\n{K}")
print(f"W:\n{W}")
```

**Output**

```python
QUERY:
tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904],
        [0.6009, 0.2566, 0.7936, 0.9408, 0.1332],
        [0.9346, 0.5936, 0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739, 0.2666, 0.6274],
        [0.2696, 0.4414, 0.2969, 0.8317, 0.1053],
        [0.2695, 0.3588, 0.1994, 0.5472, 0.0062],
        [0.9516, 0.0753, 0.8860, 0.5832, 0.3376]])
KEY:
tensor([[0.8090, 0.5779, 0.9040, 0.5547, 0.3423],
        [0.6343, 0.3644, 0.7104, 0.9464, 0.7890],
        [0.2814, 0.7886, 0.5895, 0.7539, 0.1952],
        [0.0050, 0.3068, 0.1165, 0.9103, 0.6440],
        [0.7071, 0.6581, 0.4913, 0.8913, 0.1447],
        [0.5315, 0.1587, 0.6542, 0.3278, 0.6532],
        [0.3958, 0.9147, 0.2036, 0.2018, 0.2018]])
W:
tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4714, 0.5286, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3804, 0.4184, 0.2011, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3075, 0.3105, 0.2372, 0.1448, 0.0000, 0.0000, 0.0000],
        [0.1946, 0.2316, 0.1961, 0.1527, 0.2249, 0.0000, 0.0000],
        [0.1794, 0.1895, 0.1756, 0.1363, 0.1986, 0.1206, 0.0000],
        [0.2261, 0.2320, 0.1125, 0.0699, 0.1630, 0.1312, 0.0653]])
```

Notemos ahora cómo las posiciones inválidas tienen el valor de $0$. De esta forma, cuando multipliquemos esta matriz con la matriz $V$, las posiciones inválidas no tendrán efecto, impidiendo que los tokens futuros afecten a los tokens anteriores.


### 2.2.2 Implementacion final

Ahora pasaremos con la implementación final de este módulo usando `Pytorch`. 

```python
class Head(nn.Module):

    def __init__(self,
                 n_embd:int,
                 head_size:int,
                 dropout:float,
                 context_length:int):
        super().__init__()
        self.key   = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        B,T,C = x.shape
        k = self.key(x)   # (B,T,C)
        q = self.query(x) # (B,T,C)
        
        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)
        wei = F.softmax(wei, dim=-1) # (B, T, T)
        wei = self.dropout(wei)
        
        v = self.value(x) # (B,T,C)
        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)
        return out
```

Notemos que esta implementación maneja una dimensión extra, que es el `batch_size`. Sin embargo, las operaciones siguen siendo las mismas a nivel matricial. Además, agregamos la capa de `dropout`, que nos ayuda a reducir el overfitting dentro del bloque.



## 2.3 MultiHeadAttention y FeedForward

Esta parte es más directa, ya que, como se puede observar en el paper, lo que se hace es no tener un solo módulo de atención. En su lugar, se utilizan diferentes representaciones de estos módulos para que, al final, puedas concatenarlos y obtener una representación más robusta de la atención.

Para esto, lo que hacemos es básicamente reutilizar nuestra clase `Head` y crear un arreglo completo de este módulo, para luego concatenar el resultado y procesarlo a través de una capa `nn.Linear`.


```python
class MultiHeadAttention(nn.Module):

    def __init__(self,
                 n_embd:int,
                 num_heads:int,
                 head_size:int,
                 dropout:float,
                 context_length:int):
        super().__init__()
        self.heads = nn.ModuleList([Head(n_embd,head_size,dropout,context_length) for _ in range(num_heads)])
        self.proj = nn.Linear(n_embd, n_embd)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out
```

Para la parte del `FeedForward`, también se utilizan operaciones básicas como la multiplicación de matrices y la función $ReLU$.


```python
class FeedFoward(nn.Module):

    def __init__(self,
                 n_embd:int,
                 dropout:float):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        return self.net(x)
```

Finalmente, implementamos el bloque del **Encoder** del paper utilizando estos dos layers. Este bloque es muy sencillo de implementar, ya que, considerando los layers anteriores, solo tenemos que combinarlos usando **Residual connection** y `nn.LayerNorm` para estabilizar las distribuciones entre las capas.

Algo importante a notar es que, al igual que en el paper, estamos considerando que la dimensión de cada cabeza es igual a la dimensión de los embeddings dividida entre la cantidad de cabezas que tenemos. Esto hace que la complejidad total de tener varias cabezas sea similar a la de tener una sola.


```python
class Block(nn.Module):

    def __init__(self,
                 n_embd:int,
                 n_head:int,
                 dropout:float,
                 context_length:int):
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_embd,n_head,head_size,dropout,context_length)
        self.ffwd = FeedFoward(n_embd,dropout)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x
```



## 2.4 Language Model

Ahora pasaremos a la construcción del modelo de lenguaje, teniendo en cuenta todo lo explicado previamente. Para esto, explicaremos paso a paso cuáles son los objetivos de esta red neuronal y cuáles serán los tipos de entradas. Además, detallaremos el proceso desde que tenemos cadenas de texto hasta la predicción del siguiente token.



<div class="mermaid">
    graph TD;
    subgraph "Tokenization";
    A[Input Text] --> B[BasicTokenizer];
    B --> |Convert characters to tokens| C[Token Sequence];
    C --> |Convert to tensor| D[Input Tensor idx];
    end;
    subgraph "Embedding";
    D --> E[Token Embedding Table];
    E --> |Embed tokens| F[Token Embeddings];
    G[Positional Encoding] --> |Add position info| F;
    end;
    subgraph "Transformer Blocks";
    F --> H[Multi-Head Attention];
    H --> |Compute Attention| I[Self-Attention Layer];
    I --> |Key, Query, Value Projections| J[Attention Computation];
    J --> |Masked Softmax| K[Attention Probabilities];
    K --> |Weighted Aggregation| L[Attention Output];
    L --> M[Feed Forward Network];
    M --> |Non-linear Transformation| N[Residual Connections];
    N --> O[Layer Normalization];
    end;
    subgraph "Prediction";
    O --> P[Language Model Head];
    P --> |Convert to Logits| Q[Softmax];
    Q --> |Probability Distribution| R[Token Sampling];
    R --> |Select Next Token| S[Generated Token];
    S --> |Append to Sequence| T{Continue Generation?};
    T -->|Yes| D;
    T -->|No| U[Final Output Text];
    end;
</div>

### 2.4.1 Tokenizer

Empecemos por el **Tokenizer**. Esta es una parte importante para convertir los tokens en palabras. En nuestra implementación, utilizaremos una versión básica. En esta versión, extraeremos todos los tipos de caracteres y asignaremos un número entero a cada sílaba, que será nuestra unidad de token.

```python
class BasicTokenizer:

    def __init__(self,text=None,path=None):
        if text is not None:
            self.string2int = {ch:i for i,ch in enumerate(set(text))}
            self.int2string = {i:ch for i,ch in enumerate(set(text))}
        elif path is not None:
            with open(path,"r") as f:
                data = json.load(f)
                self.string2int = data["string2int"]
                self.int2string = data["int2string"]
        else:
            raise ValueError("Either text or path must be provided")

    def tokenize(self,text:str):
        return [self.string2int[ch] for ch in text]
    
    def detokenize(self,tokens:list[int]):
        return "".join([self.int2string[i] for i in tokens])
    
    def __len__(self):
        return len(self.string2int)
    
    def get_vocab_size(self):
        return len(self.string2int)

    def save_tokenizer(self,path:str):
        with open(path,"w") as f:
            json.dump({"string2int":self.string2int,"int2string":self.int2string},f,indent=4)
```

Debemos considerar, sin embargo, una variable importante, la cual será el `vocab_size`. Esta variable representa la cantidad de tokens que existen en nuestro **Tokenizer**, es decir, la cantidad de IDs que tenemos en total. Esta variable es crucial, ya que nos servirá para construir el **Embedding** y, además, será utilizada para determinar cuál debe ser el tamaño del vector de salida de la última capa para obtener la distribución de salida de tokens.

### 2.4.2 Prediction Layer

Para el modelo de lenguaje, luego de pasar por todos los bloques de **Encoders**, finalmente pasamos por la última capa para predecir el siguiente token. Esta última capa consiste en un `nn.LayerNorm` y un `nn.Linear`, junto con la función $softmax$ para obtener la distribución de probabilidad del siguiente `token`.

Notemos que, para la última capa, debemos tener la dimensión de salida igual al tamaño del `vocab_size`.

### 2.4.3 Arquitectura, entrenamiento e inferencia

Ahora veremos la implementación final de nuestro modelo de lenguaje en código, utilizando los bloques anteriores. Además, explicaremos cuál es la función de pérdida que se utiliza, los tipos de entradas y salidas que debe tener el modelo y cómo realizamos la inferencia, es decir, las predicciones.


```python
class LanguageModel(nn.Module):
    def __init__(self,
                 vocab_size:int,
                 context_length:int,
                 n_embd:int,
                 n_head:int,
                 n_layer:int,
                 dropout:float,
                 device:torch.device):
        
        super().__init__()
        
        self.vocab_size = vocab_size
        self.context_length = context_length
        self.n_embd = n_embd
        self.n_head = n_head
        self.n_layer = n_layer
        self.dropout = dropout

        self.token_embedding_table = nn.Embedding(vocab_size,n_embd)
        self.blocks = nn.Sequential(*[Block(n_embd,n_head,dropout,context_length) for _ in range(n_layer)])
        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd,vocab_size)
        self.device = device

    def positional_encoding(self,seq_len:int, d_model:int):
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)

        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe = torch.zeros(seq_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        return pe

    def forward(self,idx,targets=None):
        B,T = idx.shape
        token_embeddings = self.token_embedding_table(idx)
        position_embeddings = self.positional_encoding(T,self.n_embd).to(self.device)
        x = token_embeddings + position_embeddings
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)

        if targets is None:
            loss = None
            return logits,loss
        else:
            loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))
            return logits,loss
        
    def generate(self, idx, max_new_tokens):
        for _ in range(max_new_tokens):
            idx_cond = idx[:, -self.context_length:]
            logits, _ = self(idx_cond)
            probs = F.softmax(logits[:, -1, :], dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=-1)

            yield idx_next.item()
```

#### 2.4.3.1 Entrenamiento

Debemos notar que, para la parte de entrenamiento, es importante definir correctamente cuáles serán nuestras entradas y salidas del modelo. Como explicamos inicialmente, el propósito es que, dada una cierta cantidad de tokens iniciales, debemos ser capaces de predecir el siguiente. Podemos ver este proceso como:

$$
[t_1,t_2,t_3,t_{N}]  \to \text{LLM} \to [t_{N+1}]
$$

Sin embargo, una pregunta válida es: ¿cuántos tokens puede recibir como entrada? Para esto, definiremos una variable `context_length`, que básicamente es un número entero predefinido que define cuántos tokens de entrada puede recibir como máximo nuestro modelo para realizar la predicción del siguiente token.

Principalmente, este valor se utiliza, por ejemplo, en `Head`, específicamente en nuestro **Masking**, donde construimos esta matriz triangular que tiene por dimensión `context_length` x `context_length`.

En nuestra implementación, no usamos de forma directa la función softmax para la última capa de nuestra red, ya que, en la implementación oficial de `PyTorch`, estos resultados ya son procesados previamente por la función $softmax$.

[PyTorch CrossEntropyLoss documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)

Esta fórmula se utiliza principalmente para tareas de clasificación y es exactamente lo que haremos con la última capa: clasificar cuál debería ser el siguiente token.

#### 2.4.3.2 Inferencia

Para la parte de inferencia, notemos que, luego de procesar la entrada por la red neuronal, solo capturamos la información del último token, que es el que estamos prediciendo: `logits[:,-1,:]`. Luego, aplicamos la función $softmax$ para obtener la distribución de los tokens. Finalmente, aplicamos la función `torch.multinomial` para muestrear en base a la distribución obtenida. Después, debemos concatenar el nuevo token obtenido a los anteriores, pero manteniendo la cantidad de tokens de entrada determinada por `context_length`.


## 2.5 Resultados

En nuestro repositorio oficial encontrarás los scripts necesarios para entrenar esta arquitectura. Puedes modificar los hiperparámetros según los recursos que tengas disponibles.  

Para facilitar la recolección de datos, utilizamos una librería que nos permite extraer texto de archivos PDF ubicados en una carpeta específica y entrenar el modelo con estos datos. Para ejecutar el script, basta con indicar dicha carpeta y los hiperparámetros correspondientes.  

A continuación, puedes ver un ejemplo de entrenamiento durante 3 horas en un dataset de aproximadamente 2 millones de tokens:  

![loss|500x500](https://res.cloudinary.com/dtpiuha91/image/upload/v1738348059/Screenshot_from_2025-01-31_12-53-06_o8huzq.png)  

Además, para el texto en español, entrenamos el modelo con diversos libros del famoso escritor peruano Mario Vargas Llosa. Si bien la inferencia no es perfecta, el modelo tiende a generar palabras con sentido y cierta coherencia. Esto podría deberse al tiempo de entrenamiento o al escalamiento del modelo.  

En futuros blogs, entrenaremos arquitecturas más complejas y actuales para analizar los resultados de inferencia en este tipo de modelos.  

![Inferencia|800x800](https://res.cloudinary.com/dtpiuha91/video/upload/v1738346271/Screencast_from_31-01-25_12_53_57_w1wt7j.webm)
