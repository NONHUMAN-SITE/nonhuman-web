# 1. Introduction

In this section, we will explain the implementation of the fundamental parts of the Transformer and how we can use it to create a basic language model. We will not implement the model used for the translation task described in the original paper, as our focus is to demonstrate how this work serves as the foundation for building language models like ChatGPT.

All the code and scripts implemented in this section are available in the official NONHUMAN repository:

[NONHUMAN Repository](https://github.com/NONHUMAN-SITE/MIND/tree/main/1.AttentionIsAllYouNeed)

Here, you will also find detailed instructions for installing the necessary dependencies and training this language model on a folder with PDF files of your choice. The hardware requirements are not high; a graphics card with at least 6 GB of RAM is sufficient. Additionally, if you have more resources, you can modify the hyperparameters to optimize the training time.

## 1.1 Objective

I believe that, before starting, it is important to consider what the objective of our language model should be. The idea is that, if we have a sequence of tokens, the model should learn to predict the next one. Furthermore, we want it to have an autoregressive behavior, meaning that this predicted token will be used to predict the next one. We can express this as:

$$
t_N = LLM(\{t_i\}_{i=1}^{N-1}) \to t_{N+1} = LLM(\{t_i\}_{i=1}^{N})
$$

However, later we will explore an important concept like `context_length`, which will be the variable that tells us how many tokens our language model can use at most for inference.

# 2. Architecture

Before reviewing the training algorithm, we will address the architecture of our language model, focusing primarily on the Transformer.

## 2.1 Positional Encoding + Embedding

As mentioned in the paper *Attention is All You Need*, there are two main components: **Embedding** and **Positional Encoding**. In the previous section, we already discussed the importance of each.

### 2.1.1 Embedding

Regarding **Embedding**, its relevance lies in how we can obtain the vector representation of each token as efficiently as possible. It functions as a *look-up table*, where we can associate each token with a specific vector.

To implement this, we need two important variables: `num_embeddings` and `embedding_dim`.

* `num_embeddings`: This is the number of tokens we will store in the *embedding*.
* `embedding_dim`: This is the final dimension of our vector.

Thus, we can visualize the **Embedding** as a matrix of dimensions `num_embeddings` x `embedding_dim`. The process is that, for each token, we retrieve its corresponding index and extract the associated vector, meaning we select the corresponding row in the matrix.

You can find more details on the implementation in PyTorch at the following link:

[PyTorch `torch.nn.Embedding` Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)

### 2.1.2 Positional Encoding

As we saw in the previous blog, due to the nature of the transformer, we need to insert information about the position of the tokens since we won't be using RNNs, which capture this information naturally. In the paper, the proposed solution is as follows:

$$
PE(pos,2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
$$

$$
PE(pos,2_{i+1}) = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
$$

where $pos$ is the position and $i$ is the corresponding dimension. We can see $PE$ as a matrix of dimensions `num_tokens` x `embedding_dim`. The implementation of this is found in the `model.py` script, as a method of the language model class, where it takes the embedding dimension and the number of tokens that will be input.

```python
def positional_encoding(self, seq_len: int, d_model: int):
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)

        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe = torch.zeros(seq_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        return pe
```

### 2.1.3 Initial Processing

Taking these two parts of the model into account, the process is as follows: after using the **Tokenizer** (which we will discuss later) and obtaining the token indices, we process them using the embedding to get their vector representation. Subsequently, we add the **Positional Encoding** matrix to this matrix to incorporate the corresponding positional information.

We can express this more formally. Let $I \in \mathbb{R}^{T}$, where $I$ will be the list of the $T$ tokens. Then:

$$
X = Embedding(I) + PE
$$

where:

$$
X \in \mathbb{R}^{N \times d_{\text{model}}}
$$

## 2.2 Head

Now we will talk about the implementation of the attention mechanism of the transformer. We will begin by implementing the class for a single `Head`, and then we will reuse this module to implement the `MultiHead`.

Once we have processed the tokens through **Embedding** and **Positional Encoding**, we obtain the vectorial and positional representation of our tokens, $X$. Next, we will implement the attention mechanism. For this, we need to obtain three different representations of our matrix $X$, which are $Query$, $Key$, and $Value$. To do this, it is enough to obtain them via a linear transformation.

$$
Q = X.W_Q \quad \text{where } W_Q \in \mathbb{R}^{d_{model} \times d_k}
$$

$$
K = X.W_K \quad \text{where } W_K \in \mathbb{R}^{d_{model} \times d_k}
$$

$$
V = X.W_V \quad \text{where } W_V \in \mathbb{R}^{d_{model} \times d_v}
$$

To implement these types of transformations, we can simply use PyTorch's `nn.Linear` layer, setting the `bias=False` parameter.

[PyTorch nn.Linear documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)

Next, the implementation of the operation to compute **Attention** is quite straightforward, as it involves basic operations such as matrix multiplication and division by a scalar.

$$
Attention(Q,K,V) = softmax\left(\frac{QK^T}{d_k^{1/2}}\right)V
$$

However, one might think that simply multiplying these matrices and performing these operations is enough. Nevertheless, as mentioned in the paper, we must apply a process called **Masking**.


### 2.2.1 Masking

As we explained in the previous blog, this process is used so that, when computing the attention between all the tokens, that is, once the operations of

$$
\frac{QK^T}{d_k^{1/2}}
$$

are performed, the result is a matrix of dimension $N \times N$. However, as we saw in the previous blog, this matrix contains information about the attention between all tokens, because matrix multiplication is essentially the dot product between vectors. Thus, each component $i,j$ of this matrix will be a dot product between row $Q_i$ and row $K_j$, as we see it transposed.

However, we must ask ourselves: is this valid? Notice that we are also capturing the attention between $token_i$ and $token_j$ with $j > i$; that is, $token_i$ is paying attention to a token that is in the future, which should not be possible, since our system works in such a way that we need to predict the next token based only on the previous ones. Therefore, we must apply a **masking** technique to make this possible. Let's first analyze the result in code.


```python
import torch
import torch.nn.functional as F
torch.manual_seed(42)

N = 7
d_k = 5

Q = torch.rand((N,d_k))
K = torch.rand((N,d_k))

W = Q @ K.T
W = F.softmax(W, dim=1)

print(f"QUERY:\n{Q}")
print(f"KEY:\n{K}")
print(f"W:\n{W}")
```

**Output**
```python
QUERY:
tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904],
        [0.6009, 0.2566, 0.7936, 0.9408, 0.1332],
        [0.9346, 0.5936, 0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739, 0.2666, 0.6274],
        [0.2696, 0.4414, 0.2969, 0.8317, 0.1053],
        [0.2695, 0.3588, 0.1994, 0.5472, 0.0062],
        [0.9516, 0.0753, 0.8860, 0.5832, 0.3376]])
KEY:
tensor([[0.8090, 0.5779, 0.9040, 0.5547, 0.3423],
        [0.6343, 0.3644, 0.7104, 0.9464, 0.7890],
        [0.2814, 0.7886, 0.5895, 0.7539, 0.1952],
        [0.0050, 0.3068, 0.1165, 0.9103, 0.6440],
        [0.7071, 0.6581, 0.4913, 0.8913, 0.1447],
        [0.5315, 0.1587, 0.6542, 0.3278, 0.6532],
        [0.3958, 0.9147, 0.2036, 0.2018, 0.2018]])
W:
tensor([[0.1866, 0.2118, 0.1440, 0.0839, 0.2004, 0.0822, 0.0910],
        [0.1972, 0.2211, 0.1397, 0.0883, 0.1824, 0.1035, 0.0677],
        [0.2220, 0.2442, 0.1174, 0.0688, 0.1546, 0.1190, 0.0739],
        [0.1897, 0.1915, 0.1463, 0.0893, 0.1487, 0.1152, 0.1193],
        [0.1570, 0.1868, 0.1582, 0.1232, 0.1814, 0.0962, 0.0972],
        [0.1588, 0.1677, 0.1555, 0.1207, 0.1758, 0.1067, 0.1148],
        [0.2261, 0.2320, 0.1125, 0.0699, 0.1630, 0.1312, 0.0653]])
```

Notice that our resulting matrix `W` contains attention information that the $token_i$ gives to the $token_j$. To solve this problem, and as mentioned in the paper, we need to eliminate these invalid values and enforce that the $token_i$ only looks at the tokens that are $\leq i$.

To do this, we will set the invalid positions to $-\infty$, so that when applying $softmax$, these become $0$. In code, this can be seen as:


```python
import torch
import torch.nn.functional as F
torch.manual_seed(42)

N = 7
d_k = 5

Q = torch.rand((N,d_k))
K = torch.rand((N,d_k))

W = Q @ K.T
tril = torch.tril(torch.ones(N, N))
W = W.masked_fill(tril == 0, float('-inf'))
W = F.softmax(W, dim=1)

print(f"QUERY:\n{Q}")
print(f"KEY:\n{K}")
print(f"W:\n{W}")
```

**Output**

```python
QUERY:
tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904],
        [0.6009, 0.2566, 0.7936, 0.9408, 0.1332],
        [0.9346, 0.5936, 0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739, 0.2666, 0.6274],
        [0.2696, 0.4414, 0.2969, 0.8317, 0.1053],
        [0.2695, 0.3588, 0.1994, 0.5472, 0.0062],
        [0.9516, 0.0753, 0.8860, 0.5832, 0.3376]])
KEY:
tensor([[0.8090, 0.5779, 0.9040, 0.5547, 0.3423],
        [0.6343, 0.3644, 0.7104, 0.9464, 0.7890],
        [0.2814, 0.7886, 0.5895, 0.7539, 0.1952],
        [0.0050, 0.3068, 0.1165, 0.9103, 0.6440],
        [0.7071, 0.6581, 0.4913, 0.8913, 0.1447],
        [0.5315, 0.1587, 0.6542, 0.3278, 0.6532],
        [0.3958, 0.9147, 0.2036, 0.2018, 0.2018]])
W:
tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4714, 0.5286, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3804, 0.4184, 0.2011, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3075, 0.3105, 0.2372, 0.1448, 0.0000, 0.0000, 0.0000],
        [0.1946, 0.2316, 0.1961, 0.1527, 0.2249, 0.0000, 0.0000],
        [0.1794, 0.1895, 0.1756, 0.1363, 0.1986, 0.1206, 0.0000],
        [0.2261, 0.2320, 0.1125, 0.0699, 0.1630, 0.1312, 0.0653]])
```

Notice now how the invalid positions have a value of $0$. In this way, when we multiply this matrix with the matrix $V$, the invalid positions will have no effect, preventing future tokens from affecting previous tokens.


### 2.2.2 Final Implementation

Now we will proceed with the final implementation of this module using `Pytorch`. 

```python
class Head(nn.Module):

    def __init__(self,
                 n_embd:int,
                 head_size:int,
                 dropout:float,
                 context_length:int):
        super().__init__()
        self.key   = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        B,T,C = x.shape
        k = self.key(x)   # (B,T,C)
        q = self.query(x) # (B,T,C)
        
        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)
        wei = F.softmax(wei, dim=-1) # (B, T, T)
        wei = self.dropout(wei)
        
        v = self.value(x) # (B,T,C)
        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)
        return out
```

Notice that this implementation handles an extra dimension, which is the `batch_size`. However, the operations are still the same at the matrix level. Additionally, we add the `dropout` layer, which helps reduce overfitting within the block.

## 2.3 MultiHeadAttention and FeedForward

This part is more straightforward, as seen in the paper, what is done is not having a single attention module. Instead, different representations of these modules are used so that, in the end, you can concatenate them and obtain a more robust representation of attention.

For this, what we do is basically reuse our `Head` class and create a full array of this module, then concatenate the result and process it through a `nn.Linear` layer.

```python
class MultiHeadAttention(nn.Module):

    def __init__(self,
                 n_embd:int,
                 num_heads:int,
                 head_size:int,
                 dropout:float,
                 context_length:int):
        super().__init__()
        self.heads = nn.ModuleList([Head(n_embd,head_size,dropout,context_length) for _ in range(num_heads)])
        self.proj = nn.Linear(n_embd, n_embd)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out
```

Para la parte del `FeedForward`, también se utilizan operaciones básicas como la multiplicación de matrices y la función $ReLU$.


```python
class FeedFoward(nn.Module):

    def __init__(self,
                 n_embd:int,
                 dropout:float):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        return self.net(x)
```
Finally, we implemented the **Encoder** block from the paper using these two layers. This block is very simple to implement, since, considering the previous layers, we just need to combine them using **Residual connection** and `nn.LayerNorm` to stabilize the distributions between layers.

An important thing to note is that, just like in the paper, we are considering that the dimension of each head is equal to the dimension of the embeddings divided by the number of heads we have. This makes the total complexity of having multiple heads similar to having just one.


```python
class Block(nn.Module):

    def __init__(self,
                 n_embd:int,
                 n_head:int,
                 dropout:float,
                 context_length:int):
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_embd,n_head,head_size,dropout,context_length)
        self.ffwd = FeedFoward(n_embd,dropout)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x
```



## 2.4 Language Model

Now we will move on to the construction of the language model, taking into account everything explained previously. To do this, we will explain step by step the objectives of this neural network and what types of inputs will be used. Additionally, we will detail the process from the moment we have text strings to the prediction of the next token.


<div class="mermaid">
    graph TD;
    subgraph "Tokenization";
    A[Input Text] --> B[BasicTokenizer];
    B --> |Convert characters to tokens| C[Token Sequence];
    C --> |Convert to tensor| D[Input Tensor idx];
    end;
    subgraph "Embedding";
    D --> E[Token Embedding Table];
    E --> |Embed tokens| F[Token Embeddings];
    G[Positional Encoding] --> |Add position info| F;
    end;
    subgraph "Transformer Blocks";
    F --> H[Multi-Head Attention];
    H --> |Compute Attention| I[Self-Attention Layer];
    I --> |Key, Query, Value Projections| J[Attention Computation];
    J --> |Masked Softmax| K[Attention Probabilities];
    K --> |Weighted Aggregation| L[Attention Output];
    L --> M[Feed Forward Network];
    M --> |Non-linear Transformation| N[Residual Connections];
    N --> O[Layer Normalization];
    end;
    subgraph "Prediction";
    O --> P[Language Model Head];
    P --> |Convert to Logits| Q[Softmax];
    Q --> |Probability Distribution| R[Token Sampling];
    R --> |Select Next Token| S[Generated Token];
    S --> |Append to Sequence| T{Continue Generation?};
    T -->|Yes| D;
    T -->|No| U[Final Output Text];
    end;
</div>

### 2.4.1 Tokenizer

Let's start with the **Tokenizer**. This is an important part for converting tokens into words. In our implementation, we will use a basic version. In this version, we will extract all character types and assign an integer to each syllable, which will be our token unit.


```python
class BasicTokenizer:

    def __init__(self,text=None,path=None):
        if text is not None:
            self.string2int = {ch:i for i,ch in enumerate(set(text))}
            self.int2string = {i:ch for i,ch in enumerate(set(text))}
        elif path is not None:
            with open(path,"r") as f:
                data = json.load(f)
                self.string2int = data["string2int"]
                self.int2string = data["int2string"]
        else:
            raise ValueError("Either text or path must be provided")

    def tokenize(self,text:str):
        return [self.string2int[ch] for ch in text]
    
    def detokenize(self,tokens:list[int]):
        return "".join([self.int2string[i] for i in tokens])
    
    def __len__(self):
        return len(self.string2int)
    
    def get_vocab_size(self):
        return len(self.string2int)

    def save_tokenizer(self,path:str):
        with open(path,"w") as f:
            json.dump({"string2int":self.string2int,"int2string":self.int2string},f,indent=4)
```

We must, however, consider an important variable, which will be the `vocab_size`. This variable represents the number of tokens present in our **Tokenizer**, that is, the total number of IDs we have. This variable is crucial, as it will be used to construct the **Embedding** and, furthermore, it will be used to determine the size of the output vector of the last layer to obtain the token output distribution.

### 2.4.2 Prediction Layer

For the language model, after passing through all the **Encoders** blocks, we finally go through the last layer to predict the next token. This last layer consists of an `nn.LayerNorm` and an `nn.Linear`, along with the $softmax$ function to obtain the probability distribution of the next `token`.

Note that, for the last layer, we need the output dimension to be equal to the size of the `vocab_size`.

### 2.4.3 Architecture, Training, and Inference

Now we will see the final implementation of our language model in code, using the previous blocks. Additionally, we will explain the loss function used, the types of inputs and outputs the model should have, and how we perform inference, that is, predictions.



```python
class LanguageModel(nn.Module):
    def __init__(self,
                 vocab_size:int,
                 context_length:int,
                 n_embd:int,
                 n_head:int,
                 n_layer:int,
                 dropout:float,
                 device:torch.device):
        
        super().__init__()
        
        self.vocab_size = vocab_size
        self.context_length = context_length
        self.n_embd = n_embd
        self.n_head = n_head
        self.n_layer = n_layer
        self.dropout = dropout

        self.token_embedding_table = nn.Embedding(vocab_size,n_embd)
        self.blocks = nn.Sequential(*[Block(n_embd,n_head,dropout,context_length) for _ in range(n_layer)])
        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd,vocab_size)
        self.device = device

    def positional_encoding(self,seq_len:int, d_model:int):
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)

        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe = torch.zeros(seq_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        return pe

    def forward(self,idx,targets=None):
        B,T = idx.shape
        token_embeddings = self.token_embedding_table(idx)
        position_embeddings = self.positional_encoding(T,self.n_embd).to(self.device)
        x = token_embeddings + position_embeddings
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)

        if targets is None:
            loss = None
            return logits,loss
        else:
            loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))
            return logits,loss
        
    def generate(self, idx, max_new_tokens):
        for _ in range(max_new_tokens):
            idx_cond = idx[:, -self.context_length:]
            logits, _ = self(idx_cond)
            probs = F.softmax(logits[:, -1, :], dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=-1)

            yield idx_next.item()
```

#### 2.4.3.1 Training

We must note that, for the training part, it is important to correctly define what our model's inputs and outputs will be. As we explained initially, the goal is that, given a certain number of initial tokens, we should be able to predict the next one. We can see this process as:

$$
[t_1,t_2,t_3,t_{N}]  \to \text{LLM} \to [t_{N+1}]
$$

However, a valid question is: how many tokens can it receive as input? For this, we will define a variable `context_length`, which is essentially a predefined integer that defines how many tokens the model can take as input at most to predict the next token.

Mainly, this value is used, for example, in `Head`, specifically in our **Masking**, where we construct this triangular matrix with dimensions `context_length` x `context_length`.

In our implementation, we do not directly use the softmax function for the last layer of our network, as, in the official `PyTorch` implementation, these results are already processed beforehand by the $softmax$ function.

[PyTorch CrossEntropyLoss documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)

This formula is mainly used for classification tasks, and it’s exactly what we will do with the last layer: classify what the next token should be.

#### 2.4.3.2 Inference

For the inference part, note that, after processing the input through the neural network, we only capture the information of the last token, which is the one we are predicting: `logits[:,-1,:]`. Then, we apply the $softmax$ function to obtain the token distribution. Finally, we apply the `torch.multinomial` function to sample based on the obtained distribution. Afterward, we need to concatenate the new token obtained with the previous ones, but keeping the number of input tokens determined by `context_length`.

## 2.5 Results

In our official repository, you will find the necessary scripts to train this architecture. You can modify the hyperparameters according to the resources you have available.

To facilitate data collection, we use a library that allows us to extract text from PDF files located in a specific folder and train the model with this data. To run the script, simply specify the folder and the corresponding hyperparameters.

Below, you can see an example of training for 3 hours on a dataset of approximately 2 million tokens:

![loss|500x500](https://res.cloudinary.com/dtpiuha91/image/upload/v1738348059/Screenshot_from_2025-01-31_12-53-06_o8huzq.png)

Also, for the Spanish text, we trained the model with various books from the famous Peruvian writer Mario Vargas Llosa. Although the inference is not perfect, the model tends to generate words with meaning and some coherence. This could be due to the training time or the scaling of the model.

In future blogs, we will train more complex and current architectures to analyze the inference results in these types of models.

![Inference|800x800](https://res.cloudinary.com/dtpiuha91/video/upload/v1738346271/Screencast_from_31-01-25_12_53_57_w1wt7j.webm)
