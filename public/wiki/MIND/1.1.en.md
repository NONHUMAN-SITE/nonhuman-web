# 1. Introduccion
En el ámbito de la inteligencia artificial, las redes neuronales recurrentes (RNNs) y las redes neuronales convolucionales (CNNs) han desempeñado un papel crucial en tareas como el procesamiento del lenguaje natural (NLP) y el reconocimiento de patrones en imágenes. Estas arquitecturas, aunque revolucionarias en su momento, enfrentaban limitaciones significativas que impulsaron la búsqueda de alternativas más eficientes y efectivas, culminando en la creación de los Transformers.

**REDES NEURONALES RECURRENTES (RNNs)**

Las RNNs fueron diseñadas específicamente para manejar datos secuenciales, como texto, audio o series temporales. Su estructura permite que la salida de un paso temporal alimente al siguiente, utilizando un estado oculto que transporta información contextual a lo largo de la secuencia. Sin embargo, esta arquitectura presenta las siguientes limitaciones principales:

1. **Computación lenta para secuencias largas:** Debido a su naturaleza secuencial, las RNNs procesan una palabra a la vez, lo que ralentiza tanto el entrenamiento como la inferencia.
2. **Desvanecimiento y explosión del gradiente:** Durante el entrenamiento, los gradientes pueden volverse demasiado pequeños (vanishing) o demasiado grandes (exploding), afectando la capacidad del modelo para aprender dependencias de largo plazo. Soluciones como LSTM y GRU mitigaron parcialmente este problema, pero no lo resolvieron completamente.
3. **Dificultad para acceder a información antigua:** Aunque las RNNs mantienen un estado oculto que transmite información previa, tienden a priorizar el contexto reciente, olvidando elementos importantes de contextos más lejanos.

* *Ejemplo:*

-En una tarea de predicción de la próxima palabra en la oración *"el gato persigue al ratón"*, una RNN podría olvidar la relación entre "gato" y "raton" si la oración es demasiado larga.

**Redes Neuronales Convolucionales (CNNs)**

Por otro lado, las CNNs, inicialmente diseñadas para el reconocimiento de imágenes, también se adaptaron para tareas de NLP. Estas redes utilizan filtros convolucionales que detectan patrones locales en los datos, como relaciones entre palabras cercanas en una oración. Sin embargo, presentan desventajas importantes en el manejo de secuencias largas:

1. **Falta de contextualización global:** Los filtros convolucionales son efectivos para captar patrones locales, pero tienen dificultades para modelar dependencias entre palabras distantes.
2. **Estructuras predefinidas:** El tamaño del filtro y el número de capas deben definirse manualmente, limitando su capacidad para adaptarse a secuencias complejas.
3. **Ineficiencia en tareas secuenciales:** Aunque son más rápidas que las RNNs gracias a su paralelismo, carecen de mecanismos intrínsecos para capturar dependencias temporales.

* *Ejemplo:*

-En un párrafo como *"El desarrollo de las redes neuronales ha revolucionado la inteligencia artificial"*, una CNN puede identificar palabras relacionadas como "desarrollo" y "redes", pero puede perder conexiones importantes entre "revolucionado" e "inteligencia artificial" debido a la distancia entre ellas.

# 2. Inputs, tokens y embeddings

Para que un modelo como el Transformer procese texto, primero debemos convertirlo a un formato numérico que el modelo pueda interpretar. Este proceso incluye dos pasos clave: **tokenización** y **embedding**.


## 2.1 Tokenizacion

La tokenización es el proceso de dividir un texto en unidades más pequeñas llamadas **tokens**. Dependiendo de la tarea y la complejidad, los tokens pueden ser:
- **Palabras**: Dividir una oración en palabras individuales. Ejemplo: “El gato duerme” → ["El", "gato", "duerme"].
- **Sub-palabras o caracteres**: Esto se usa para aligerar la carga computacional y manejar mejor palabras desconocidas. Ejemplo: “gato” → ["ga", "to"] o ["g", "a", "t", "o"].

Los métodos de tokenización más comunes incluyen:
- **Byte Pair Encoding (BPE)**: Divide las palabras en sub-unidades comunes, permitiendo un vocabulario compacto.
- **WordPiece** (usado por BERT): Similar a BPE pero optimizado para modelos de lenguaje.
- **Métodos basados en caracteres**: Usados en tareas con lenguajes de baja frecuencia o texto ruidoso.

## 2.2 Embeddings

Un **embedding** es una representación numérica de los tokens en un espacio vectorial. La idea principal detrás de los embeddings es mapear cada token a un vector de dimensiones fijas. De esta manera, los tokens similares, ya sea por su semántica o contexto, estarán más cercanos entre sí en el espacio vectorial, mientras que los tokens diferentes estarán más separados.

Por ejemplo, en un embedding, las palabras "gato" y "felino" podrían estar cercanas, mientras que "gato" y "coche" estarán más alejadas.

Para imaginar los embeddings, podemos pensar en puntos distribuidos en un espacio 2D o 3D, donde cada punto representa un token y la proximidad entre los puntos refleja la similitud semántica. Videos como el de **DotCSV** explican visualmente cómo los embeddings agrupan palabras relacionadas en clústeres dentro del espacio vectorial. Estas representaciones son esenciales para que los modelos puedan capturar relaciones entre palabras y contexto de manera efectiva.

#### **Video recomendado sobre Embeddings:**

[INTRO al Natural Language Processing (NLP) #2](https://www.youtube.com/watch?v=RkYuH_K7Fx4)

# 3. Attention

Ahora explicaremos uno de los conceptos más importantes del paper, para esto debemos de preguntarnos, ¿cómo codificamos la atención? Observemos la siguiente fórmula que nos propone el paper.

Consideremos entonces las matrices $$Q \in \mathbb{R}^{N\times d_k}, K \in \mathbb{R}^{N\times d_k}, V \in \mathbb{R}^{N\times d_v}$$ donde tenemos que $Q, K ,V$ son las matrices conocidas como Querys, Keys y Values. Estas son obtenidas a partir de una transformación lineal obtenida luego de pasar por la capa de embedding. Luego tenemos que en el paper codifican la atención como 

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Expliquemos el por qué de esta fórmula y cuál es la intuición detrás. Empecemos por lo más básico

## 3.1 Multiplicacion de matrices
La multiplicación de matrices podemos observarla de la siguiente forma en términos matemáticos, sean la matrices $Q$ y $K$ tal que tenemos que su producto, con $K$ transpuesta, tendrá la dimensión de $S \in \mathbb{R}^{N \times N}$. Tomando la fila $i$ y la columna $j$ tenemos que cada componente de $S$ se puede calcular como:

$$S_{i,j} = \sum_{k=1}^{d_k}Q_{i,k}.K^{T}_{k,j}$$

En otras palabras estamos realizando un producto interno entre la fila $i$ de la matriz $Q$ y la columna $j$ de $K^{T}$. Recordemos que podemos considerar a cada fila de $Q$ y $K$ como la información en dimensión $d_k$ de cada token, ya que tendremos $N$ de estas filas. Entonces al realizar el producto interno entre filas y columnas, estamos guardando en el componente $S_{i,j}$ la información cruzada entre el $token_i$ y el $token_j$. De esta forma, podemos almacenar en esta matriz toda la información global de las relaciones que tienen entre tokens de manera eficiente.


Sin embargo, esto aún no es suficiente. Ahora introduciremos el por qué tenemos que normalizar este resultado; es decir, dividirlo por $\sqrt{d_k}$.

## 3.2 Normalizacion

Empezemos por un caso simple, tomemos dos vectores $q \in \mathbb{R}^{d_k}$ y $v \in \mathbb{R}^{d_k}$. Supongamos que ambos tienen $\mu=0,\sigma²=1$. Entonces, al momento de realizar el producto interno, tenemos que la variable resultante tendrá $\mu=0,\sigma²=d_k$. Por lo tanto, a medida que tengamos una mayor dimensión, tendremos que el valor resultante tiene altas probabilidades de ser un valor muy negativo o muy positivo. Sin embargo, ¿por qué esto es un problema? Porque ofrece inestabilidad y más aún si queremos usar la función $softmax$. Analicemos en detalle este fenómeno.

## 3.3 Softmax

La función $softmax$ está representada como

$$
softmax(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{n}e^{x_j}}
$$


Notemos entonces que al insertar un vector en softmax tendremos que el vector resultante será una distribución de probabilidad, ya que se cumple que $\sum_{i=1}^{n}softmax(x)_i = 1$. Sin embargo, debido a la forma que tiene esta función, analicemos qué sucede cuando los componentes del vector $x$ tiene valores muy negativos y muy positivos.

Notemos que debido a que la función $softmax$ no tiene una salida escalar, entonces, su derivada está representada por el $Jacobiano$, ya que recordemos que 

$$
\frac{\partial f}{\partial v}(a) = Df(a).v
$$

donde $Df(a)$ es la matriz compuesta por todas las derivadas parciales de $f$.

Entonces, volviendo al caso de la función $softmax$, tenemos que

$$
\frac{\partial softmax(x)_i}{\partial x_j} = softmax(x)_i.(\delta_{i,j}-softmax(x)_j)
$$

donde $\delta_{i,j}$ es la delta de Kronecker.

Entonces, debido a la forma que tiene este componente del jacobiano de la función $softmax$ podemos observar que en casi todos los casos posibles este se hace casi $0$. Esto se puede demostrar tomando casos diferentes tomando algunos componentes muy positivos y algunos componentes muy negativos, y reemplazarlos en la derivada parcial de la función. 

Para métodos prácticos, mostraremos una simulación donde mostraremos que teniendo vectores aleatorios con $\mu=0,\sigma²=d_k$, tendremos que la norma del jacobiano de la función $softmax$ se hace más pequeña a medida que la dimensión $d_k$ aumenta. Esto implicaría que los gradientes se hacen muy pequeños y por lo tanto el entrenamiento no es efectivo, habiendo visto este efecto antes en secciones anteriores donde se conoce como $\text{vanishing gradient}$


```python
import numpy as np
import matplotlib.pyplot as plt

def softmax(z):
    e_z = np.exp(z - np.max(z))
    return e_z / e_z.sum()

def softmax_jacobian(z):
    s = softmax(z).reshape(-1, 1)
    return np.diagflat(s) - np.dot(s, s.T)

def compute_jacobian_norm(z):
    J = softmax_jacobian(z)
    return np.linalg.norm(J, 'fro')

num_simulations = 1000
d_k_values = np.logspace(0, 4, 20)
norms = []

for d_k in d_k_values:
    current_norms = []
    for _ in range(num_simulations):
        z = np.random.normal(loc=0, scale=np.sqrt(d_k), size=50)
        current_norms.append(compute_jacobian_norm(z))
    norms.append(np.mean(current_norms))

plt.figure(figsize=(10, 6))
plt.scatter(d_k_values, norms, alpha=0.7, color='red')
plt.xscale('log')
plt.yscale('log')
plt.xlabel('$d_k$ (escala logarítmica)', fontsize=12)
plt.ylabel('Norma de Frobenius del Jacobiano (log)', fontsize=12)
plt.title('Decaimiento de la norma del Jacobiano de Softmax con $d_k$ grande', fontsize=14)
plt.grid(True, which="both", ls="--")
```

![jacobian_dk|600x500](https://res.cloudinary.com/dtpiuha91/image/upload/fl_preserve_transparency/v1738131143/jacobian_dk_ajyekw.jpg)


Ahora veamos qué sucede cuando aplicamos el efecto normalizador, para esto, basta con solo cambiar la línea de
```python
z = np.random.normal(loc=0, scale=np.sqrt(d_k), size=50)
z = z/np.sqrt(d_k)
```

![jacobian_dk_normalized|600x500](https://res.cloudinary.com/dtpiuha91/image/upload/v1738131143/jacobian_dk_normalized_r1vitk.png)


Notemos que ahora sin importar el tamaño de la dimensión $d_k$ la norma de la matriz jacobiana se mantiene estable y uniforme sin decaimientos.


## 3.4 Attention

Ahora que ya entendimos la multiplicación de las matrices $Q$ y $K$, el efecto de la normalización por su dimensión $d_k$ y sy implicancia al momento de usar el $softmax$, llegaremos finalmente a la parte de ¿por qué se le llama un mecanismo de atención?

Definamos a la matriz resultante de la operación de $softmax$ como

$$
S = softmax(\frac{Q.K^T}{\sqrt{d_k}}) \in \mathbb{R}^{N\times N}
$$

Entonces, el siguiente paso para definir $Attention$ es la multiplicación la matriz $V \in \mathbb{R}^{N\times d_v}$. Sin embargo, la clave para entender este mecanismo de atención se encuentra cuando expresamos las filas de la matriz de $Attention$ de la siguiente forma

$$
Attention_i = \sum_{k=1}^{N}S_{i,k}V_k
$$

entonces, podemos observar que la fila $i$ de $Attention$ no es más que una suma ponderada de las filas de $V$, ya que debido a la función $softmax$ tenemos que $\sum_{k=1}^{N}S_{i,k}=1 \ \forall i = 1,2,...,N$. Entonces, intuitivamente estamos almacenando en cada fila de $Attention$ esta suma ponderada donde $S_{i,k}$ indica cual sería la relevancia de cada $token_k$ para el $token_i$ usando los "pesos" determinados por la matriz $S_{i,k}$.

# 6. Positional Encodings
En **Transformers**, uno de los desafíos clave es cómo representar el orden de las palabras. A diferencia de las redes recurrentes (RNN), el mecanismo *self-attention* procesa los tokens en paralelo, lo que elimina la secuencialidad implícita. Sin embargo, el orden de los tokens es esencial para el sentido lingüístico. Aquí es donde entran en juego los **Positional Encodings**.

## 6.1. ¿Por que necesitamos Positional Encodings?

En arquitecturas como las RNN, el orden de las palabras está integrado en el proceso recurrente, ya que las palabras se procesan secuencialmente. En cambio, en los Transformers, el procesamiento paralelo hace que el modelo pierda noción de la secuencia a menos que se introduzca explícitamente. Esto podría resultar en ambigüedades, donde frases como “Alicia come manzanas” y “Manzanas come Alicia” serían indistinguibles.

Para evitar esto, es necesario incorporar explícitamente la información de posición en los embeddings de entrada. Un buen método para este propósito debe cumplir con los siguientes criterios:
1. Producir una codificación única para cada posición de la palabra en la oración.
2. Mantener la consistencia en las distancias entre posiciones, independientemente de la longitud de las secuencias.
3. Generalizar a secuencias más largas que las vistas durante el entrenamiento.
4. Ser determinista y no depender de aprendizaje adicional.


## 6.2 Fundamentos del Encoding sinusoidal

Según Vaswani et al. (*Attention Is All You Need*) y se discute también en *Transformer Architecture: The Positional Encoding (Kazemnejad, 2019)*, los Positional Encodings se calculan usando funciones sinusoidales, que garantizan una representación continua y única para cada posición:

$$
PE(pos, 2i)   = \sin(pos \cdot \omega_i), \quad PE(pos, 2i+1) = \cos(pos \cdot \omega_i)
$$

Donde:

$$
\omega_i = 1 / (10000^{2i / d_{model}})
$$

Estas funciones alternan senos y cosenos con frecuencias decrecientes a lo largo de las dimensiones del vector, creando una "firma" única para cada posición. La codificación resultante se suma a los embeddings de los tokens, manteniendo la dimensionalidad fija.

---

## 6.3 Intuicion basica

El encoding sinusoidal puede entenderse como una versión continua de un sistema binario. En un contador binario, cada bit cambia más lentamente que el anterior: el bit menos significativo alterna en cada paso, el siguiente cambia cada dos pasos, y así sucesivamente. De manera similar, en el encoding sinusoidal, las frecuencias más altas representan posiciones cercanas, mientras que las frecuencias más bajas reflejan relaciones a mayor distancia.

Esta superposición de frecuencias genera un patrón único para cada posición, lo que permite al modelo distinguirlas de forma efectiva.

---

## 6.4 Propiedades clave del encoding sinusoidal

El encoding sinusoidal tiene propiedades que lo hacen particularmente adecuado para Transformers:
- **Relaciones relativas:** Permite que el modelo aprenda fácilmente relaciones posicionales relativas, ya que \\( PE(pos + k) \\) puede representarse como una función lineal de \\( PE(pos) \\).
- **Simetría y decaimiento:** Las distancias entre posiciones consecutivas son simétricas y disminuyen suavemente con el tiempo, facilitando el aprendizaje de patrones a distintas escalas.
- **Generalización:** Gracias a su naturaleza periódica, el encoding puede extrapolar a secuencias más largas que las vistas en el entrenamiento.

Estas características lo convierten en un método robusto y eficiente para incorporar información posicional.

---

## 6.5 Incorporacion en el Transformer

En la arquitectura Transformer, el encoding posicional se suma al embedding de cada token antes de que sea procesado por las capas de atención:

$$
x'_{pos} = x_{pos} + PE(pos)
$$

Este diseño permite que el modelo procese simultáneamente la semántica y la posición sin aumentar la dimensionalidad ni requerir más parámetros.

### ¿Por que sumar en lugar de concatenar?

La suma asegura que la dimensionalidad del vector de entrada \\( d_{model} \\) permanezca constante, reduciendo la complejidad computacional y evitando interferencias innecesarias entre la información semántica y posicional. Además, las conexiones residuales (*skip connections*) de la arquitectura preservan la información posicional a lo largo de las capas.

# 8. Implementacion

Ahora con todo lo visto previamente, daremos paso a la implementación en código del bloque principal de atención

## Referencias

1. [Vaswani, Ashish, et al. "Attention Is All You Need". *NeurIPS*, 2017.](https://arxiv.org/abs/1706.03762)
2. [Kazemnejad, Amirhossein. "Transformer Architecture: The Positional Encoding". *kazemnejad.com*, 2019.](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
