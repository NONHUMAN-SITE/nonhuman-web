# 1. Introduction

In this section, we will explain several details of the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762). We will go part by part, breaking down each section and analyzing it in as much detail as possible. Then, in the following section [1.2](https://www.nonhuman.site/research/wiki/MIND/1.2), we will address the code implementation and how we can train a small language model.

**Problem**

In the field of artificial intelligence, recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have played a crucial role in tasks such as natural language processing (NLP) and pattern recognition in images. These architectures, although revolutionary at the time, face significant limitations that have driven the search for more efficient and effective alternatives, culminating in the creation of Transformers.

## 1.1 Recurrent Neural Networks (RNNs)

RNNs were specifically designed to handle sequential data, such as text, audio, or time series. Their structure allows the output of one time step to feed into the next, using a hidden state that carries contextual information throughout the sequence. However, this architecture has the following main limitations:

1. **Slow computation for long sequences:** Due to their sequential nature, RNNs process one word at a time, slowing down both training and inference.
2. **Vanishing and exploding gradients:** During training, gradients can become too small (*vanishing*) or too large (*exploding*), affecting the model’s ability to learn long-term dependencies. Solutions like LSTM and GRU partially mitigated this problem but did not solve it completely.
3. **Difficulty accessing old information:** While RNNs maintain a hidden state that transmits prior information, they tend to prioritize recent context, forgetting important elements from more distant contexts.

**Example**

In a task predicting the next word in the sentence:

*"The cat chases the mouse all afternoon. Despite his efforts, he doesn't manage to catch it."*

an RNN might forget the relationship between *"cat"* and *"he"* if the sentence is too long.

## 1.2 Convolutional Neural Networks (CNNs)

On the other hand, CNNs, originally designed for image recognition, have also been adapted for NLP tasks. These networks use convolutional filters that detect local patterns in the data, such as relationships between nearby words in a sentence. However, they have important drawbacks when handling long sequences:

1. **Lack of global context:** Convolutional filters are effective at capturing local patterns but struggle to model dependencies between distant words.  
2. **Predefined structures:** The filter size and number of layers must be manually defined, limiting their ability to adapt to complex sequences.  
3. **Inefficiency in sequential tasks:** While they are faster than RNNs due to their parallelism, they lack intrinsic mechanisms to capture temporal dependencies.

**Example**

In a paragraph like *"The development of neural networks has revolutionized artificial intelligence"*, a CNN might identify related words like *"development"* and *"networks"*, but could miss important connections between *"revolutionized"* and *"artificial intelligence"* due to the distance between them.

## 2. Inputs: Tokens and Embeddings

Now we will discuss basic concepts that will be useful for the following sections. These elements are responsible for initially processing the text input and enabling its mathematization. As is well known, in Deep Learning, it is necessary to convert data into mathematical units that neural networks can work with, usually represented as matrices or vectors. This process includes two key steps: **tokenization** and **embedding**.

## 2.1 Tokenization

Tokenization is the process of splitting text into smaller units called **tokens**. Depending on the task and complexity, tokens can be:

- **Words**: Splits a sentence into individual words.  
  Example: “The cat sleeps” → `["The", "cat", "sleeps"]`.
- **Subwords or characters**: Used to reduce computational load and better handle unknown words.  
  Example: “cat” → `["ca", "t"]` or `["c", "a", "t"]`.

Common tokenization methods include:

- **Byte Pair Encoding (BPE)**: Splits words into common subunits, allowing for a compact vocabulary.
- **WordPiece** (used by BERT): Similar to BPE but optimized for language models.
- **Character-based methods**: Used in tasks with low-frequency languages or noisy text.

## 2.2 Embeddings

An **embedding** is a numerical representation of tokens in a vector space. The main idea behind embeddings is to map each token to a fixed-dimensional vector. In this way, similar tokens, whether by their semantics or context, will be closer together in the vector space if we use metrics like **cosine similarity**, while different tokens will be farther apart.

$$
\text{cosine similarity}(u,v) = \frac{u \cdot v}{\|u\| \|v\|}
$$

To imagine embeddings, we can think of points distributed in a 2D or 3D space, where each point represents a token and the proximity between points reflects semantic similarity. These representations are essential for models to effectively capture relationships between words and context. The main task is for the model to learn the best vector representation of tokens. From now on, we will refer to the initial dimension of each token as $d_{\text{model}}$.

**Recommended videos on Embeddings:**

- [INTRO to Natural Language Processing (NLP) #2](https://www.youtube.com/watch?v=RkYuH_K7Fx4)
- [What are Word Embeddings?](https://www.youtube.com/watch?v=wgfSDrqYMJ4)


# 3. Attention

Once we have explained how we can convert sentences into numerical representations, we will now explain one of the most important concepts in the paper. To do this, we must ask ourselves: What does attention mean? How do we encode attention? Let's look at the following formula proposed in the paper.

Consider the matrices $Q \in \mathbb{R}^{N\times d_k}, K \in \mathbb{R}^{N\times d_k}, V \in \mathbb{R}^{N\times d_v}$, where $Q, K, V$ are the matrices known as *Queries*, *Keys*, and *Values*. These are obtained from a linear transformation after passing through the embedding layer. Then, in the paper, attention is encoded as:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{d_k^{1/2}} \right)V
$$

Let's explain the reasoning behind this formula and the intuition behind it. Let's start with the basics.

> Note: We did not use the natural square root symbol due to issues with the KaTeX renderer. We will implement a patch for this in future versions of the page.

## 3.1 Matrix Multiplication

We can express matrix multiplication in the following way mathematically. Let $Q \in \mathbb{R}^{N\times d_k}$ and $K \in \mathbb{R}^{N\times d_k}$ be the matrices, and their product $S = QK^T$ (where $K^T$ is the transpose of $K$) will have the dimension $S \in \mathbb{R}^{N \times N}$. If we take the $i$th row and the $j$th column, each component of $S$ can be computed as:

$$
S_{i,j} = \sum_{k=1}^{d_k} Q_{i,k} K^{T}_{k,j}
$$

In other words, we are performing a dot product between the $i$th row of matrix $Q$ and the $j$th row of $K$ (due to the transpose). Recall that we can consider each row of $Q$ and $K$ as the information in dimension $d_k$ of each token:

$$
\begin{bmatrix}
\text{token}_1 \\
\text{token}_2 \\
\vdots \\
\text{token}_N \\
\end{bmatrix}
$$

where each token $i$ is represented as a vector in $\mathbb{R}^{d_k}$:

$$
\text{token}_i \in \mathbb{R}^{d_k} \quad \text{(seen as a row vector)}
$$

By performing the dot product between rows and columns, we store in the component $S_{i,j}$ the cross-information between $\text{token}_i$ and $\text{token}_j$. In this way, we can efficiently represent all global information about the relationships between tokens in this matrix.

However, this is still not enough. Now we will explain why it is necessary to normalize this result, that is, to divide it by $$d_k^{1/2}$$.

## 3.2 Normalization

Let’s start with a simple case. Let’s take two vectors $q \in \mathbb{R}^{d_k}$ and $v \in \mathbb{R}^{d_k}$. Suppose both have $\mu=0$ and $\sigma^2=1$. Then, when performing the dot product, the resulting variable will have $\mu=0$ and $\sigma^2=d_k$. Therefore, as the dimension $d_k$ increases, the resulting value is highly likely to be either very negative or very positive.

Why is this a problem? Because it introduces instability, especially if we want to use the $softmax$ function, which is used in our attention formula. Let’s analyze this phenomenon in detail.

## 3.3 $Softmax$

The $softmax$ function is represented as:

$$
\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
$$

Notice that when applying $softmax$ to a vector, the result is a probability distribution, since it holds that:

$$
\sum_{i=1}^{n} \text{softmax}(x)_i = 1
$$

However, due to the form of this function, let’s analyze what happens when the components of the vector $x$ take very negative and very positive values.

Since the $softmax$ function does not have a scalar output, its derivative is represented by the \textit{Jacobian}, as we recall that:

$$
\frac{\partial f}{\partial v}(a) = Df(a) \cdot v
$$

where $Df(a)$ is the matrix composed of all the partial derivatives of $f$.

In the case of the $softmax$ function, we have:

$$
\frac{\partial \text{softmax}(x)_i}{\partial x_j} = \text{softmax}(x)_i \cdot (\delta_{i,j} - \text{softmax}(x)_j)
$$

where $\delta_{i,j}$ is the Kronecker delta.

Due to the form of this component of the Jacobian of the $softmax$ function, we can observe that in most cases its value is close to $0$. This can be demonstrated by considering different cases where some components of the vector $x$ are very positive and others very negative, and substituting them into the partial derivative of the function.

To illustrate this phenomenon practically, we will perform a simulation where we generate random vectors with $\mu=0$ and $\sigma^2=d_k$, and analyze how the norm of the Jacobian of the $softmax$ function reduces as the dimension $d_k$ increases. This implies that the gradients become very small, making it difficult to train the model. This effect, already analyzed in previous sections, is known as the \textit{vanishing gradient} problem.

```python
import numpy as np
import matplotlib.pyplot as plt

def softmax(z):
    e_z = np.exp(z - np.max(z))
    return e_z / e_z.sum()

def softmax_jacobian(z):
    s = softmax(z).reshape(-1, 1)
    return np.diagflat(s) - np.dot(s, s.T)

def compute_jacobian_norm(z):
    J = softmax_jacobian(z)
    return np.linalg.norm(J, 'fro')

num_simulations = 1000
d_k_values = np.logspace(0, 4, 20)
norms = []

for d_k in d_k_values:
    current_norms = []
    for _ in range(num_simulations):
        z = np.random.normal(loc=0, scale=np.sqrt(d_k), size=50)
        current_norms.append(compute_jacobian_norm(z))
    norms.append(np.mean(current_norms))

plt.figure(figsize=(10, 6))
plt.scatter(d_k_values, norms, alpha=0.7, color='red')
plt.xscale('log')
plt.yscale('log')
plt.xlabel('$d_k$ (logarithmic scale)', fontsize=12)
plt.ylabel('Frobenius norm of Jacobian (log)', fontsize=12)
plt.title('Decay of the Softmax Jacobian Norm with Large $d_k$', fontsize=14)
plt.grid(True, which="both", ls="--")
```

![Decay of the Softmax Jacobian Norm with Large $d_k$|600x600](https://res.cloudinary.com/dtpiuha91/image/upload/v1738512076/o7mhdnunansizcp6c2oo.png)

Now let's see what happens when we apply the normalizing effect. To do this, simply change the line of code:

```python
z = np.random.normal(loc=0, scale=np.sqrt(d_k), size=50)
z = z/np.sqrt(d_k)
```

![Decay of the Softmax Jacobian Norm with Large $d_k$|600x600](https://res.cloudinary.com/dtpiuha91/image/upload/v1738512068/swldmsr929ol7lh2kca1.png)

Notice that now, regardless of the dimension size $d_k$, the Jacobian matrix norm remains stable and uniform without decay.

## 3.4 Mechanism

Now that we understood the multiplication of the matrices $Q$ and $K$, the effect of normalization by the dimension $d_k$, and its implications when using $softmax$, we finally reach the part of why it is called an attention mechanism.

We define the matrix resulting from the $softmax$ operation as

$$
S = \text{softmax}\left(\frac{Q K^T}{d_k^{1/2}}\right) \in \mathbb{R}^{N \times N}
$$

The next step to define attention ($Attention$) is to multiply by the matrix $V \in \mathbb{R}^{N \times d_v}$. However, the key to understanding this attention mechanism lies in expressing the rows of the attention matrix as follows:

$$
Attention_i = \sum_{k=1}^{N} S_{i,k} V_k
$$

We can observe that the row $i$ of $Attention$ is nothing more than a weighted sum of the rows of $V$, as due to the $softmax$ function, we have $\sum_{k=1}^{N} S_{i,k} = 1 \ \forall i = 1,2,...,N$. Intuitively, we are storing in each row of $Attention$ this weighted sum, where $S_{i,k}$ indicates the relevance of each $token_k$ for $token_i$ using the "weights" determined by the matrix $S_{i,k}$.

# 4. Self-Attention

In the previous section, we explained the attention mechanism and its mathematical foundations. Now, we will focus on what makes **Self-Attention** unique: the fact that the matrices $Q$, $K$, and $V$ come from the same input sequence. This property allows each token to "look" at all others in the sequence in one step, which facilitates modeling long-range dependencies and parallelization.

## 4.1 From Attention to Self-Attention

Self-Attention follows the same formulation of the attention equation described earlier, but with the particularity that the **Query ($Q$), Key ($K$), and Value ($V$)** matrices come from the same input sequence:

$$
Q = X W_q, \quad K = X W_k, \quad V = X W_v
$$

That is:

$$
\text{Self-Attention}(X) = \text{softmax}\left( \frac{X W_q (X W_k)^T}{d_k^{1/2}} \right) X W_v
$$

Where:

- $X \in \mathbb{R}^{N \times d_\text{model}}$ is the input embedding matrix, where $N$ is the number of tokens in the sequence and $d_\text{model}$ is the embedding dimension.
- $W_q, W_k, W_v \in \mathbb{R}^{d_\text{model} \times d_k}$ are learnable weight matrices.

This feature allows each token to attend to all other tokens in the sequence, **including itself**, simultaneously. That is, instead of relying on previous states, as in an RNN, each token can directly access the information from all other tokens in the sequence.

## 4.2 Contextual Modeling

One of the main benefits of this approach is its ability to capture dependencies within a local context, which allows for better understanding of relationships between nearby words in a sequence. Each token can access relevant information from the entire sequence in one step, regardless of its position, resulting in highly efficient processing.


**Example of Context in NLP**

Let's imagine the sentence:

> "The bank approved the loan because it had sufficient funds."

In a Transformer, the **Self-Attention** mechanism allows the word *bank* to receive information from both *loan* and *funds*, helping to disambiguate whether *bank* refers to a financial institution or a physical object. The attention matrix $S$ visualizes these relationships, showing how the tokens influence the contextual interpretation of the sentence.

In addition to improving local contextualization, it is also effective at capturing long-range dependencies in a sequence. This is achieved through the attention matrix $S = QK^T$, which evaluates the similarity between tokens. By applying $softmax$, the values are normalized and attention weights are calculated, allowing each token to efficiently access relevant information from distant tokens. This solves the **vanishing or exploding gradient** problem in RNNs, ensuring effective information propagation without loss in learning.

The **Self-Attention** mechanism can generate more *interpretable* models. By inspecting attention distributions, we can observe how different attention heads learn to perform specific tasks, often related to the **syntactic** and **semantic** structure of sentences.

## 4.3 Comparison and Analysis of Its Architecture

The following table from the paper illustrates the architectural advantages at the computational complexity level:

| **Layer Type**                | **Complexity per layer**    | **Sequential Operations**    | **Maximum Dependency Length**  |
|-------------------------------|-----------------------------|------------------------------|--------------------------------|
| **Self-Attention**             | $O(n^2 \cdot d)$            | $O(1)$                        | $O(1)$                         |
| **Recurrent (RNN)**            | $O(n \cdot d^2)$            | $O(n)$                        | $O(n)$                         |
| **Convolutional (CNN)**        | $O(k \cdot n \cdot d^2)$    | $O(1)$                        | $O(\log_k(n))$                 |
| **Self-Attention (restricted)**| $O(r \cdot n \cdot d)$      | $O(1)$                        | $O(n/r)$                       |

### Computational Complexity

A layer connects all positions in the sequence with a computational complexity of $O(n^2 \cdot d)$, where $n$ is the sequence length and $d$ is the embedding dimension. This operation is efficient for sequences of moderate length, as attention matrices are computed in parallel, allowing all tokens to be processed simultaneously, optimizing GPU usage and reducing training times. This facilitates handling large data batches and training models with long sequences more efficiently.

In comparison:

- **Recurrent**:  
  **RNNs** have a complexity of $O(n \cdot d^2)$ because information must propagate sequentially (one token at a time) through hidden states, making them less efficient, especially when the sequence length is large.  
- **Convolutional**:  
  On the other hand, **CNNs** have a complexity of $O(k \cdot n \cdot d^2)$, where $k$ is the filter size. While they are faster than RNNs for long sequences, their ability to capture long-range dependencies is limited by the filter size, making them less effective than **Self-Attention**.

### Improvement in Long Sequences

Although **Self-Attention** is efficient for short sequences, its quadratic complexity $O(n^2)$ can be a challenge for long sequences. A solution is restricted attention, where each token is only connected to a local neighborhood of size $r$, reducing the complexity to $O(r \cdot n \cdot d)$. This improves efficiency without sacrificing too much of the ability to learn important dependencies, balancing computational efficiency and the capture of long-range dependencies.

### Maximum Dependency Length in **Self-Attention**

In **Self-Attention**, the maximum dependency length is $O(1)$, meaning that each token can attend to all others directly, regardless of their position in the sequence. This allows it to efficiently capture global relationships.

In comparison:

- **RNN**: The dependency length is $O(n)$, meaning that information must propagate sequentially through hidden states.
- **CNN**: The dependency length depends on the filter size and is $O(\log_k(n))$, which limits the capture of long-range dependencies without increasing the network depth.
- **Restricted Self-Attention**: Limits the dependency to a local neighborhood of size $O(n/r)$, improving efficiency for long sequences.

# 5. Multi-Head Attention: Attending to Multiple Perspectives

In the traditional **attention** mechanism, each token in the sequence is related to other tokens based on their similarity, using the *scaled dot-product attention* operation. However, when this process is done with a single attention head, the model may struggle to capture different meanings and complex relationships between words. For example, in the sentence *“The bank approved the loan”*, the word *bank* could mean a financial institution or a seat. Relying on the correct context is key to avoiding ambiguities.

To address this limitation, the **Multi-Head Attention** mechanism divides the attention into multiple parallel "heads", allowing each one to focus on different semantic and grammatical aspects of the sequence. For example, one head might identify noun relationships, such as between "bank" and "loan", while another could focus on adjectival meaning or how polysemous words are disambiguated.


## 5.1 Definition and Mathematical Formulation

**Multi-Head Attention** is based on the idea of performing multiple attention operations in parallel, where each one works with lower-dimensional projections of the original space. Mathematically, each attention head is defined as:

$$
\text{head}_i = \text{Attention}(Q W^Q_i, K W^K_i, V W^V_i)
$$

where the projections are parameter matrices learned during training:

- $ W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k} $
- $ W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k} $
- $ W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v} $
- $ W^O \in \mathbb{R}^{h d_v \times d_{\text{model}}} $

These matrices project the inputs $ Q $, $ K $, and $ V $ into lower-dimensional subspaces, allowing each head to process information independently. The relationship between the dimensions to maintain the same complexity as using a single head is as follows:

$$
d_k = d_v = \frac{d_\text{model}}{h}
$$

Finally, the results of all heads are concatenated and projected back to the original space through a matrix $ W^O $:

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
$$

In this way, the model can learn and capture multiple relationships between words within the sequence from different representation subspaces. **Multi-Head Attention** allows the model to jointly attend to information from different subspaces at various positions, avoiding the loss of information that would occur if only a single attention head were used.

## 5.2 Processing Flow and Explanation of the Image

Next, we explain the flow of **Multi-Head Attention** using the attached image:

![Multi-Head Attention](https://res.cloudinary.com/dtpiuha91/image/upload/v1738463404/multiheadattention_hapac4.png)

First, the input sequence, represented by a matrix of dimensions $ \text{seq} \times d_\text{model} $, passes through three linear projections, transforming into the matrices $ Q $, $ K $, and $ V $. These projections are essential for calculating attention, as they represent the tokens from different semantic perspectives.

The resulting matrices $ Q $, $ K $, and $ V $ are split into multiple attention "heads." Each head operates in a lower-dimensional subspace $ d_k = d_\text{model}/h $, allowing the model to process multiple relationships simultaneously. This step is crucial for capturing both short-range relationships and long-term dependencies.

Each head calculates attention using the *scaled dot-product attention* formula:

$$
\text{Attention}(Q', K', V') = \text{softmax}\left(\frac{Q' {K'}^T}{d_k^{1/2}}\right)V'
$$

The dot product calculation between $ Q' $ and $ K' $ measures the similarity between tokens, and the result is normalized using the $ \text{softmax} $ function. This ensures that each token is related in a weighted manner to the others, assigning more weight to the more relevant tokens.

Once attention is calculated in each head, the results are concatenated and projected to the original space through the matrix $ W^O $. This results in an enriched representation of the sequence, in which multiple semantic relationships have been captured.

---

## 5.3 Explanation of the Graph and Advantages of Multi-Head Attention

The attached graph illustrates the complete flow of the **Multi-Head Attention** mechanism, from the initial input to the final enriched representation. This mechanism optimizes the way the Transformer captures relationships between tokens within a sequence, allowing multiple "heads" to work independently and then combine their results. Below, we detail the essential steps of the process.


### Step 1: Initial Input and Projection in Matrices $Q$, $K$, and $V$

The input sequence, represented as a matrix of dimensions $\text{seq} \times d_\text{model}$, is projected through linear transformations into three matrices: $Q$, $K$, and $V$. These projections convert the tokens into internal representations that facilitate the evaluation of their contextual and semantic relationships.

### Step 2: Division into Multiple Heads

The matrices $Q$, $K$, and $V$ are divided into several independent "attention heads." The image shows 6 heads, although this number can vary depending on the model configuration. Each head works in a lower-dimensional subspace $d_k = d_\text{model}/h$ and specializes in capturing specific relationships. For example, one head might focus on syntactic relationships such as subject and predicate, while another might capture associations between adjectives and nouns.

* Advantage: This division process allows the model to capture multiple perspectives of the context without interference. For example, one head may identify grammatical relationships, while another focuses on disambiguating polysemous words. This specialization is crucial for tasks where meanings heavily depend on context.

---

### Step 3: Calculation of *Scaled Dot-Product Attention*

Each head performs the dot product between its projected matrices $Q'$ and $K'$ to measure the similarity between tokens. The result is divided by $ d_k^{1/2}$ to avoid extreme values that could affect the performance of the $softmax$ and its gradients. Then, the $softmax$ function transforms these similarities into a probability distribution, assigning more weight to the most relevant tokens.

$$
\text{Attention}(Q', K', V') = \text{softmax}\left(\frac{Q' {K'}^T}{d_k^{1/2}}\right)V'
$$

### Step 4: Weighting of Values $V'$ 

The attention matrix resulting from the $softmax$ is used to weight the values in $V'$, generating a contextualized representation of each token. In this way, each token incorporates relevant information from other tokens in the sequence. For example, if the word *bank* is found in the context of *loan*, the weighting will favor its interpretation as a financial institution.

* Advantage: This weighting allows the model to capture both local and long-distance relationships efficiently, without the need to process the sequence sequentially as in RNNs. Thus, each token can directly access relevant information from other tokens, solving the problem of vanishing information in long sequences.

### Step 5: Concatenation and Final Projection

The outputs of all attention heads are concatenated and passed through a final linear projection via the matrix $W^O$. This generates a final representation of the sequence, where multiple semantic, grammatical, and contextual relationships are combined.

Advantage: The combination of multiple heads allows synthesizing information from different perspectives, generating a rich and complete representation. This process improves performance in complex tasks, such as machine translation or syntactic dependency analysis, by integrating both specific relationships and global contexts.


# 6. Positional Encodings

In **Transformers**, one of the key challenges is how to represent the order of words. Unlike recurrent networks (RNNs), the **self-attention** mechanism processes the tokens in parallel, which eliminates implicit sequentiality. However, the order of the tokens is essential for linguistic meaning. This is where **Positional Encodings** come into play.

## 6.1. Why Do We Need Positional Encodings?

In architectures like RNNs, the word order is integrated into the recurrent process since the words are processed sequentially. In contrast, in Transformers, the parallel processing makes the model lose track of the sequence unless explicitly introduced. This could result in ambiguities, where phrases like "Alicia eats apples" and "Apples eat Alicia" would be indistinguishable.

To avoid this, it is necessary to explicitly incorporate positional information into the input embeddings. A good method for this purpose should meet the following criteria:
1. Produce a unique encoding for each word's position in the sentence.
2. Maintain consistency in the distances between positions, regardless of sequence length.
3. Generalize to longer sequences than those seen during training.
4. Be deterministic and not require additional learning.

## 6.2. Fundamentals of Sinusoidal Encoding

According to Vaswani et al. (*Attention Is All You Need*) and also discussed in *Transformer Architecture: The Positional Encoding* (Kazemnejad, 2019), the Positional Encodings are calculated using sinusoidal functions, which guarantee a continuous and unique representation for each position:

$$
PE(pos, 2i) = \sin(pos \cdot \omega_i), \quad PE(pos, 2i+1) = \cos(pos \cdot \omega_i)
$$

Where:

$$
\omega_i = 1 / (10000^{2i / d_{model}})
$$

These functions alternate between sines and cosines with decreasing frequencies across the vector dimensions, creating a unique "signature" for each position. The resulting encoding is added to the token embeddings, maintaining the fixed dimensionality.


## 6.3 Basic Intuition

Sinusoidal encoding can be understood as a continuous version of a binary system. In a binary counter, each bit changes more slowly than the previous one: the least significant bit alternates every step, the next one changes every two steps, and so on. Similarly, in sinusoidal encoding, higher frequencies represent nearby positions, while lower frequencies reflect relationships at greater distances.

This overlap of frequencies generates a unique pattern for each position, allowing the model to distinguish them effectively.

Here’s an effective video on how we can intuitively understand how this mechanism works:

[Positional encoding video](https://www.youtube.com/watch?v=T3OT8kqoqjc)


## 6.4 Key Properties of Sinusoidal Encoding

Sinusoidal encoding has properties that make it particularly suitable for Transformers:
- **Relative relationships:** It allows the model to easily learn relative positional relationships, as $$ PE(pos + k) $$ can be represented as a linear function of $$ PE(pos) $$.
- **Symmetry and decay:** The distances between consecutive positions are symmetric and decay smoothly over time, making it easier to learn patterns at different scales.
- **Generalization:** Due to its periodic nature, the encoding can extrapolate to longer sequences than those seen during training.

These characteristics make it a robust and efficient method for incorporating positional information.

## 6.5 Incorporation in the Transformer

In the Transformer architecture, the positional encoding is added to the embedding of each token before it is processed by the attention layers:

$$
x'_{pos} = x_{pos} + PE(pos)
$$

This design allows the model to process both the semantics and the position simultaneously without increasing the dimensionality or requiring additional parameters.

**Why add instead of concatenate?**

Addition ensures that the input vector's dimensionality $$ d_{model} $$ remains constant, reducing computational complexity and avoiding unnecessary interference between semantic and positional information. Moreover, the [residual connections](https://arxiv.org/pdf/1512.03385) of the architecture preserve the positional information across layers.

# 7. Masking

**Masking** is used to prevent a token from attending to future tokens; that is, future tokens have no effect on the prediction of a previous token, thus preserving the autoregressive property. In the scaled dot-product attention phase, masks are introduced that assign the value $-\infty$ to the positions corresponding to future tokens. This way, when applying $softmax$, these values become 0, eliminating any influence from tokens that should not be considered yet.

## 7.1 Mask Representation

If we consider the interaction between queries $$ Q $$ and keys $$ K $$ for a sequence of length $$ N $$, the score matrix is modified as follows:

$$
\begin{bmatrix}
a_{1,1} & -\infty & -\infty & \cdots & -\infty \\
a_{2,1} & a_{2,2} & -\infty & \cdots & -\infty \\
a_{3,1} & a_{3,2} & a_{3,3} & \cdots & -\infty \\
\vdots  & \vdots  & \vdots  & \ddots & \vdots \\
a_{N,1} & a_{N,2} & a_{N,3} & \cdots & a_{N,N}
\end{bmatrix}
$$

Note that each component of row $$ i $$ can be viewed as the effect of each token $$ j $$ on token $$ i $$, as when multiplying

$$
Q.K^T
$$

we have $$ a_{i,j} = Q_i.K_j $$, where the subscript indicates the row number. Therefore, the component $$ a_{i,j} $$ stores the attention that token $$ i $$ gives to token $$ j $$.

In this way, by applying this masking, we ensure that only tokens before the corresponding token have an effect. In the next blog, we will take a closer look at the implementation of this technique.

## 7.2 Effect of $Softmax$

After applying the $softmax$ function, the $$ -\infty $$ values are transformed into $$ 0 $$, resulting in a matrix where future tokens do not influence attention. For example, the resulting attention matrix would look like this:

$$
\begin{bmatrix}
p_{1,1} & 0 & 0 & \cdots & 0 \\
p_{2,1} & p_{2,2} & 0 & \cdots & 0 \\
p_{3,1} & p_{3,2} & p_{3,3} & \cdots & 0 \\
\vdots  & \vdots  & \vdots  & \ddots & \vdots \\
p_{N,1} & p_{N,2} & p_{N,3} & \cdots & p_{N,N}
\end{bmatrix}
$$

# 8. Conclusion

In this blog, we have seen the main concepts behind the Transformer model, from attention to positional encodings and masking. In the next blog, we will explore how we can implement all of the above in code and also go over the process for training a small language model from scratch.

[Implementation Blog](https://www.nonhuman.site/research/wiki/MIND/1.2)

## References

1. [Vaswani, Ashish, et al. "Attention Is All You Need". *NeurIPS*, 2017.](https://arxiv.org/abs/1706.03762)
2. [Kazemnejad, Amirhossein. "Transformer Architecture: The Positional Encoding". *kazemnejad.com*, 2019.](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
3. [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909v5)
4. [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385).

